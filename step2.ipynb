{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection â€“ pay extra attention on how to find this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ['N', 'N', 'N', 'N', 'N']\n",
    "E = ['R', 'R', 'R', 'R', 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ['R', 'N', 'R', 'N', 'R']\n",
    "E = ['N', 'R', 'N', 'R', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_precision(rankings):    \n",
    "    \"\"\"Calculates Average Precision (AP) = Average of precisions at relevant documents\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The average precision of rankings\n",
    "    \"\"\"\n",
    "    relevant = 0\n",
    "    numerator = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        if rel == 'R' or rel == 'HR':\n",
    "            relevant += 1\n",
    "            numerator += relevant/rank\n",
    "    return numerator/len(rankings)\n",
    "\n",
    "average_precision(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\"\"\"\n",
    "The scores for relevance that are used are 0, 1 and 5 for N, R and HR respectively (equal to example in slides).\n",
    "\n",
    "The nDCG@k measures requires the total number of relevant and highly relevant documents in the entire collection. Since\n",
    "for this dummy example there does not really exist a corpus, another approach is required:\n",
    "\n",
    "def nDCGk is feeded a ranking of length 5 (one of the permutations created in Step 1). This list of five is treated\n",
    "as the corpus. The list consisting of the first k elements is seen as the result of a query q, and the DCGk of this\n",
    "list is calculated by def DCGk. To find the perfect ranking (for normalization), the corpus list is sorted (descending)\n",
    "and the DCGk of the top k elements of the resulting list is calculated. The result is then used for normalization \n",
    "in def nDCGk.\n",
    "\"\"\"\n",
    "\n",
    "def DCGk(rankings):\n",
    "    \"\"\"Calculates Discounted Cumulative Gain (DCG)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The discounted cumulative gain.\n",
    "    \"\"\"\n",
    "    discounted_gain = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        gain = (2**rel)-1\n",
    "        discount = 1/math.log(rank+1,2)    \n",
    "        discounted_gain += gain*discount        \n",
    "    return discounted_gain\n",
    "\n",
    "def nDCGk(rankings, k=3):\n",
    "    \"\"\"Calculates Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query. (treated as corpus)\n",
    "        k (int): rank k. Value of 3 is used if no argument is given.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized discounted gain at rank k.\n",
    "    \"\"\"\n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5 if x is 'HR' else 1 if x is 'R' else 0 for x in items]\n",
    "    \n",
    "    # calculate discounted gain for top k\n",
    "    DCG = DCGk(rankings[:k])\n",
    "    # sort all relevant documents (descending) in the corpus by their \n",
    "    # relative relevance to find best possible DCG result (perfect ranking)\n",
    "    perfect_DCG = DCGk(sorted(rankings, reverse=True)[:k])\n",
    "    # normalize discounted_gain by this result\n",
    "    return DCG/perfect_DCG\n",
    "\n",
    "#nDCGk(['HR', 'HR', 'N','N','R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8333333333333334, 0.8333333333333334, 0, 0, 0.16666666666666666] \n",
      "\n",
      "\n",
      "1 0.8333333333333334\n",
      "2\n",
      "******************************\n",
      "2 0.8333333333333334\n",
      "4\n",
      "******************************\n",
      "3 0\n",
      "8\n",
      "******************************\n",
      "4 0\n",
      "16\n",
      "******************************\n",
      "5 0.16666666666666666\n",
      "32\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# expect Ri to be an increasing function of the relevance grade,\n",
    "# N  = 0\n",
    "# R  = 1/6\n",
    "# HR = 5/6\n",
    "\n",
    "def ERR(rankings):\n",
    "    #Expected Reciprocal Rank (ERR).\n",
    "\n",
    "    \n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5/6 if x is 'HR' else 1/6 if x is 'R' else 0 for x in items]\n",
    "    \n",
    "    print(rankings,'\\n\\n')\n",
    "\n",
    "    \n",
    "    prob = 1.0\n",
    "    \n",
    "\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        print(rank,rel)\n",
    "        #prob *= (1-rel)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #utility = (pow(2, rel) - 1) / pow(2, max_grade)\n",
    "        \n",
    "        utility = ((2**rank))\n",
    "        \n",
    "        print(utility)\n",
    "        #print(prob)\n",
    "        print('*'*30)\n",
    "    \n",
    "\n",
    "ERR(['HR', 'HR', 'N','N','R'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
