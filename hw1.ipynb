{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "## 1) Hypothesis Testing – The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this: <br>\n",
    "A. Modify/Build an algorithm<br>\n",
    "B. Compare the algorithm to a baseline by running a hypothesis test.<br>\n",
    "C. If not significant, go back to step A<br>\n",
    "D. If significant, start writing a paper. <br>\n",
    "<br>\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = α):<br>\n",
    "<br>\n",
    "a) P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "\n",
    "b) P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a) If all m experiments lack the power reject the null hypothesis, we still assume that the results come from the probability distribution of H0. The probability of a significant result is given by the probability mass to the right of the critical value, corrected as the family-wise error rate.\n",
    " \n",
    "P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>) = $$1 - (1-\\alpha)^m$$\n",
    "\n",
    "b) The probability of finding at least one significant result is the complement of finding no significant results.\n",
    "\n",
    "P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>) =\n",
    "\n",
    "$$1 - (1-\\alpha)^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bias and unfairness in Interleaving experiments [10 points]\n",
    "\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ⅔ of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(inspired by: 'Large Scale Validation and Analysis of Interleaved Search Evaluation' by Chapelle et al.)\n",
    "\n",
    "Consider an ambiguous query q.\n",
    "\n",
    "Of all users that search for *q*, 49% intend concept *A*, 49% intend concept *B* and 2% intend concept *C*. These users would be satisfied by docs *a*, *b* or *c* respectively.\n",
    "\n",
    "\n",
    "Consider two rankings: *R<sub>1</sub>(a, b, d)* and *R<sub>2</sub>(b, c, d)*, where d is not relevant.\n",
    "R<sub>1</sub> satisfies 98% of all users (users intending *A* + users intending *B* = 49% + 49% = 98%), whereas R<sub>2</sub> satisfies 51% of all users (users intending *B* + users intending *C* = 49% + 2% = 51%). So R<sub>1</sub> is better than R<sub>2</sub> since R<sub>2</sub> satisfies a larger portion of users.\n",
    "\n",
    "Because R<sub>1</sub> is better than R<sub>2</sub>, one would expect that R<sub>1</sub> wins more often than R<sub>2</sub> when using team-draft interleaving. However, R<sub>2</sub> will win in 51% of the encounters.\n",
    "\n",
    "This is because:\n",
    "\n",
    "* In the case of a user that is satisfied by doc *a* (49% of users), R<sub>1</sub> will always win since R<sub>2</sub> does not include doc *a*.\n",
    "* In the case of a user that is satisfied by doc *b* (49% of users), two things can happen. Either R<sub>1</sub> is chosen first, or R<sub>2</sub> is chosen first. In both cases, R<sub>2</sub> contributes doc *b* to the interleaved result, and therefore R<sub>2</sub> wins.\n",
    "* In the case of a user that is satisfied by doc *c* (2% of users), R<sub>2</sub> always wins, since R<sub>1</sub> does not include doc *c*.\n",
    "\n",
    "Therefore, R<sub>1</sub> contributes a relevant click in 49% of the cases (users searching for doc *a*) while R<sub>2</sub> contributes a relevant click in 51% of the cases (users searching for doc *b* and users searching for doc *c*).\n",
    "\n",
    "Concluding: even though R<sub>1</sub> satisfies 98% of all users and R<sub>2</sub> only 51% of users, R<sub>2</sub> will win 51% of the time when using team-draft interleaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical part\n",
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "The function ```generate_ranking_pairs``` defined below is used to generate all possible ranking pairs of a predefined length for a given set of relevance labels, which will be used later to compare offline and online evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible ranking pairs:  58806\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    " \n",
    "def generate_ranking_pairs(grades, rank_len):\n",
    "    \"\"\" Generates all possible ranking pairs, excluding pairs of \n",
    "        equal rankings\n",
    "\n",
    "    Args:\n",
    "        grades (list): Possible relevance labels in ranking.\n",
    "        rank_len (int): Length of ranking pairs.\n",
    "\n",
    "    Returns:\n",
    "        generator: All possible ranking pairs.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # generate all possible rankings\n",
    "    rankings = product(grades, repeat=rank_len)\n",
    "    # generate all possible pairs of rankings\n",
    "    pairs = product(rankings, repeat=2)    \n",
    "    # exclude pairs of equal rankings\n",
    "    pairs = filter(lambda pair: pair[0] != pair[1], pairs)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "relevance_labels = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "\n",
    "ranking_pairs = list(generate_ranking_pairs(relevance_labels, rank_len))\n",
    "print('Total number of possible ranking pairs: ', len(ranking_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "In this section, several offline evaluation measures are implemented. The first one, average precision, is a simple binary evaluation method, which is calculated as the average of precisions at relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_precision(rankings):    \n",
    "    \"\"\"Calculates Average Precision (AP) = Average of precisions at relevant documents\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The average precision of rankings.\n",
    "        \n",
    "    \"\"\"\n",
    "    relevant = 0\n",
    "    numerator = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        if rel == 'R' or rel == 'HR':\n",
    "            relevant += 1\n",
    "            numerator += relevant/rank\n",
    "    return numerator/len(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to some multi-graded evaluation measures:\n",
    "\n",
    "The scores for relevance labels that are used are 0, 1 and 5 for N, R and HR respectively (equal to example in slides).\n",
    "\n",
    "The nDCG@k measures requires the total number of relevant and highly relevant documents in the entire collection. Since\n",
    "for this dummy example there does not really exist a corpus, another approach is required:\n",
    "\n",
    "The ```nDCGk``` function is fed a ranking of length 5 (one of the permutations created in Step 1). This list of five is treated\n",
    "as the corpus. The list consisting of the first k elements is seen as the result of a query q, and the DCGk of this\n",
    "list is calculated by the ```DCGk``` function. To find the perfect ranking (for normalization), the corpus list is sorted (descending)\n",
    "and the DCGk of the top k elements of the resulting list is calculated. The result is then used for normalization \n",
    "in ```nDCGk```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def DCGk(rankings):\n",
    "    \"\"\"Calculates Discounted Cumulative Gain (DCG)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The discounted cumulative gain.\n",
    "        \n",
    "    \"\"\"\n",
    "    discounted_gain = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        gain = (2**rel)-1\n",
    "        discount = 1/math.log(rank+1,2)    \n",
    "        discounted_gain += gain*discount        \n",
    "    return discounted_gain\n",
    "\n",
    "def nDCGk(rankings, k=3):\n",
    "    \"\"\"Calculates Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query. (treated as corpus)\n",
    "        k (int): rank k. Value of 3 is used if no argument is given.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized discounted gain at rank k.\n",
    "        \n",
    "    \"\"\"\n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5 if x is 'HR' else 1 if x is 'R' else 0 for x in rankings]\n",
    "    # calculate discounted gain for top k\n",
    "    DCG = DCGk(rankings[:k])\n",
    "    # sort all relevant documents (descending) in the corpus by their \n",
    "    # relative relevance to find best possible DCG result (perfect ranking)\n",
    "    perfect_DCG = DCGk(sorted(rankings, reverse=True)[:k])\n",
    "    # normalize discounted_gain by this result\n",
    "    if perfect_DCG == 0:\n",
    "        return 0\n",
    "    return DCG/perfect_DCG\n",
    "\n",
    "#nDCGk(ranking_pairs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected reciprocal rank (ERR) metric supports graded relevance judgments and assumes a cascade browsing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ERR(rankings, k=5):\n",
    "    \"\"\"Computes the Expected Reciprocal Rank (ERR) metric in linear time. Based on paper by Chapelle et al.\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "        k (int): rank k. Value of 5 is used if no argument is given.\n",
    "\n",
    "    Returns:\n",
    "        float: the Expected Reciprocal Rank (ERR).\n",
    "        \n",
    "    \"\"\"\n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5 if x is 'HR' else 1 if x is 'R' else 0 for x in rankings]\n",
    "    p = 1.0\n",
    "    err = 0.0\n",
    "    for rank, rel in enumerate(rankings[:k]):\n",
    "        rank += 1\n",
    "        R = ((2**rel)-1) / (2**max(rankings))\n",
    "        err += p*(R/rank)\n",
    "        p *= 1-R\n",
    "    return err\n",
    "\n",
    "# ERR(ranking_pairs[100][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the delta measure (0 points)\n",
    "Using the offline evaluation measures we implemented above, we can quantify the difference between ranking pairs as the \"delta measure\". We'll apply this measure to the ranking pairs generated in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_measure(P, E, measure):\n",
    "    \"\"\"Computes the delta of P and E for average precision, nDCG@k or ERR.\n",
    "\n",
    "    Args:\n",
    "        P (list): ranked result of production algorithm.\n",
    "        E (list): ranked result of experimental algorithm.\n",
    "        measure (string): measure to be calculated (average precision, nDCGk, ERR)\n",
    "\n",
    "    Returns:\n",
    "        float: delta of E and P. If < 0, E does not outperform P.\n",
    "        \n",
    "    \"\"\"\n",
    "    if measure == 'average precision':\n",
    "        P_measure = average_precision(P)\n",
    "        E_measure = average_precision(E)\n",
    "    elif measure == 'nDCGk':\n",
    "        P_measure = nDCGk(P)\n",
    "        E_measure = nDCGk(E)\n",
    "    elif measure == 'ERR':\n",
    "        P_measure = ERR(P)\n",
    "        E_measure = ERR(E)\n",
    "    \n",
    "    return E_measure - P_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AP, E outperforms P in 27954 out of 58806 times.\n",
      "Using ERR, E outperforms P in 29403 out of 58806 times.\n"
     ]
    }
   ],
   "source": [
    "relevance_labels = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "\n",
    "ranking_pairs = list(generate_ranking_pairs(relevance_labels, rank_len))\n",
    "\n",
    "delta_ranking_ap = [delta_measure(pair[0], pair[1], 'average precision') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'average precision') > 0]\n",
    "delta_ranking_ndcgk = [delta_measure(pair[0], pair[1], 'nDCGk') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'nDCGk') > 0]\n",
    "delta_ranking_err = [delta_measure(pair[0], pair[1], 'ERR') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'ERR') > 0]\n",
    "\n",
    "print('Using AP, E outperforms P in {} out of {} times.'.format(len(delta_ranking_ap), len(ranking_pairs)))\n",
    "print('Using nDCGk, E outperforms P in {} out of {} times.'.format(len(delta_ranking_ndcgk), len(ranking_pairs)))\n",
    "print('Using ERR, E outperforms P in {} out of {} times.'.format(len(delta_ranking_err), len(ranking_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)\n",
    "The function ```interleave``` implements the Team Draft Interleaving algorithm, which can be applied to a pair of lists containing ranked relevance labels. It is assumed that the two lists contain different documents/urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def interleave(ranking_A, ranking_B):\n",
    "    \"\"\"\n",
    "    Interleaves rankings from two different ranking algorithms into \n",
    "    one ranking based on Team Draft Interleaving. Both rankings are\n",
    "    assumed to have equal length.\n",
    "    \n",
    "    Args:\n",
    "        ranking_A (list): Ranking of algorithm A.\n",
    "        ranking_B (list): Ranking of algorithm B.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dict containing two lists: the interleaved ranking and the origins.\n",
    "        \n",
    "    \"\"\"\n",
    "    ranking_I = []\n",
    "    origins = []\n",
    "    i = 0\n",
    "    while len(ranking_I) < len(ranking_A):\n",
    "        # A wins\n",
    "        if randint(0,1) == 0:\n",
    "            origins += [0,1]\n",
    "            ranking_I.append(ranking_A[i])\n",
    "            ranking_I.append(ranking_B[i])\n",
    "        # B wins\n",
    "        else:\n",
    "            origins += [1,0]\n",
    "            ranking_I.append(ranking_B[i])\n",
    "            ranking_I.append(ranking_A[i])\n",
    "        i += 1\n",
    "    if len(ranking_I) > len(ranking_A):\n",
    "        ranking_I = ranking_I[0:-1]\n",
    "        origins = origins[0:-1]\n",
    "    return {'ranking': ranking_I, 'origins': origins}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'R', 'HR', 'N'] ['R', 'HR', 'N', 'N', 'R']\n",
      "['R', 'N', 'HR', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "ranking_A = ['N','N','R','HR','N']\n",
    "ranking_B = ['R','HR','N','N','R']\n",
    "interleaved_ranking = interleave(ranking_A, ranking_B)\n",
    "print('Ranking A:\\t\\t', ranking_A)\n",
    "print('Ranking B:\\t\\t', ranking_B)\n",
    "print('Interleaved ranking:\\t', interleaved_ranking['ranking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement User Clicks Simulation (15 points)\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks. Two click models, the random click model and simple dependent click model, are implemented below.\n",
    "### Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "class RandomClickModel:\n",
    "    '''\n",
    "    Implements a Random Click Model. This model decides to click on a document\n",
    "    with a probability P_click without taking anything else into account. P_click\n",
    "    is learned from a click log. The default click log is from Yandex.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, click_log_path = 'YandexRelPredChallenge.txt'):\n",
    "        self.P_click = self.train(click_log_path)\n",
    "        \n",
    "    def train(self, click_log_path):\n",
    "        '''\n",
    "        Estimates the only parameter P_click of the Random Click Model using a click log\n",
    "        by dividing the total amount of clicks by the total amount of shown documents.\n",
    "\n",
    "        Args:\n",
    "            click_log_path (str): Location of the click log.\n",
    "\n",
    "        Return:\n",
    "            float: The P_click parameter used to decide whether to click on a document.\n",
    "\n",
    "        '''\n",
    "        shown_docs = 0\n",
    "        clicks = 0\n",
    "        with open(click_log_path,'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "\n",
    "                # Count all shown docs\n",
    "                if line[2] == 'Q':\n",
    "                    shown_docs += len(line)-5\n",
    "\n",
    "                # Count all clicks\n",
    "                else:\n",
    "                    clicks += 1\n",
    "        P_click = clicks/float(shown_docs)\n",
    "        return P_click\n",
    "\n",
    "    def predictProb(self, ranking):\n",
    "        '''\n",
    "        Generates the probability for each document in a ranking to be clicked on\n",
    "        based on a Random Click Model.\n",
    "\n",
    "        Args:\n",
    "            ranking (list): List of ranked documents represented by relevance.\n",
    "\n",
    "        Return:\n",
    "            list: A list of click probabilities for each document in the ranking.\n",
    "        '''\n",
    "        click_probabilities = []\n",
    "        for doc in ranking:\n",
    "            click_probabilities.append(self.P_click)\n",
    "        return click_probabilities\n",
    "\n",
    "    def assignClicks(self, ranking):\n",
    "        '''\n",
    "        Based on their click probabilities, either do or do not assign a click to each document.\n",
    "\n",
    "        Args:\n",
    "            ranking_probabilities (List): A list of click probability and document tuples.\n",
    "            P_click (float): Probability used to decide whether to click on a document.\n",
    "\n",
    "        Return:\n",
    "            list: A list representing clicks with 1's on documents in ranking with the same index.\n",
    "        '''\n",
    "        click_probabilities = self.predictProb(ranking)\n",
    "        clicks = []\n",
    "        for prob in click_probabilities:\n",
    "            if uniform(0,1) < prob:\n",
    "                clicks.append(1)\n",
    "            else:\n",
    "                clicks.append(0)\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ranking_I = ['HR','R','HR','N','R']\n",
    "clicks = RandomClickModel().assignClicks(ranking_I)\n",
    "print(clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dependent Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class SdcmClickModel(object):\n",
    "    \"\"\" Implements a Simple Dependent Click Model (SDCM). Generates clicks starting\n",
    "        from the first rank, and stochastically decides if a clicked result is \n",
    "        satisfactory, using probabilities trained on a click log. If the result was \n",
    "        not satisfactory, the next result is examined, and then clicked with a \n",
    "        probability dependent on the corresponding relevance label. The probabilities\n",
    "        for each of the relevance labels are taken from the book \"Click Models for \n",
    "        Web Search\".\n",
    "    \n",
    "    Args:\n",
    "        click_log_path (str): Location of the click log.\n",
    "        attr_model (str): Model used for attractiveness parameters\n",
    "                            (perfect/navigational/informative)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Click model attractiveness parameters\"\"\"\n",
    "    attr_perf = {'HR':1.0, 'R':0.5, 'N':0.0}\n",
    "    attr_nav = {'HR':0.95, 'R':0.5, 'N':0.05}\n",
    "    attr_inf = {'HR':0.9, 'R':0.7, 'N':0.4} \n",
    "    \n",
    "    def __init__(self, click_log_path='YandexRelPredChallenge.txt', attr_model='nav'):\n",
    "        self.click_log_path = click_log_path\n",
    "        if attr_model == 'nav':\n",
    "            self.attr_model = SdcmClickModel.attr_nav\n",
    "        elif attr_model == 'perf':\n",
    "            self.attr_model = SdcmClickModel.attr_perf            \n",
    "        elif attr_model == 'inf':\n",
    "            self.attr_model = SdcmClickModel.attr_inf\n",
    "        else:\n",
    "            raise ValueError('The attractiveness model \"{}\" is not available.' \\\n",
    "                            ' Choose from \"nav\", \"perf\" or \"inf\".'.format(attr_model))\n",
    "        self.params = self.learnParams()\n",
    "            \n",
    "    def learnParams(self):\n",
    "        \"\"\" Learns parameters for simple dependent click model.\n",
    "\n",
    "        Returns:\n",
    "            dict: Learnt click model parameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with open(self.click_log_path,'r') as f:\n",
    "            # global rank click counter\n",
    "            rank_counter = Counter()\n",
    "            # counter for clicks on consecutive ranks\n",
    "            pair_counter = Counter()\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                time_passed = line[1]\n",
    "                if time_passed == '0':\n",
    "                    prev_clicked_rank = None\n",
    "                if line[2] == 'Q':\n",
    "                    results = line[5:]\n",
    "                else:\n",
    "                    url = line[3]\n",
    "                    try:\n",
    "                        clicked_rank = results.index(url)\n",
    "                    except: \n",
    "                        pass\n",
    "                    rank_counter[clicked_rank] += 1\n",
    "                    if prev_clicked_rank is not None:\n",
    "                        pair_counter[prev_clicked_rank] += 1\n",
    "                    prev_clicked_rank = clicked_rank\n",
    "        lambdas = {r+1:pair_counter[r]/rank_counter[r] for r in pair_counter.keys()}\n",
    "        return lambdas\n",
    "\n",
    "    \n",
    "    def attrProbs(self, ranking):\n",
    "        \"\"\" Assigns attractiveness probabilities for list of relevance labels.\n",
    "\n",
    "        Args:\n",
    "            ranking (list): Ranked list of relevance labels.\n",
    "\n",
    "        Returns:\n",
    "            list: List of attractiveness probabilities corresponding with ranking.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = [self.attr_model[label] for i, label in enumerate(ranking)]\n",
    "        return probs\n",
    "\n",
    "    def assignClicks(self, ranking):\n",
    "        \"\"\" Assigns clicks based on attractiveness and satisfactoriness probabilities.\n",
    "\n",
    "        Args:\n",
    "            ranking (list): Ranked list of relevance labels.\n",
    "\n",
    "        Returns:\n",
    "            list: Assigned clicks.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = self.attrProbs(ranking)\n",
    "        \n",
    "        clicks = []  \n",
    "        for i in range(len(probs)):\n",
    "            if i == 0:\n",
    "                \n",
    "                # For first result, click probability equals attractiveness\n",
    "                p_click = probs[i]\n",
    "            else:\n",
    "                prev_click = clicks[i-1]\n",
    "                if prev_click == 1:\n",
    "                    p_satisf = 1 - self.params[i]\n",
    "                    \n",
    "                    # Stochastically decide satisfactoriness    \n",
    "                    outcome = random()\n",
    "                    if outcome < p_satisf:\n",
    "                        \n",
    "                        # When the clicked result was satisfactory, stop\n",
    "                        clicks += ([0] * (len(probs) - len(clicks)))\n",
    "                        break\n",
    "                    else:\n",
    "                        p_click = probs[i]\n",
    "                else:\n",
    "                    p_click = probs[i]\n",
    "\n",
    "            # Stochastically decide click \n",
    "            outcome = random()\n",
    "            if outcome < p_click:\n",
    "                clicks.append(1)\n",
    "            else: clicks.append(0)\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ranking = ['HR', 'R', 'R', 'N', 'HR', 'N', 'R']\n",
    "sdcm_clicks = SdcmClickModel().assignClicks(ranking)\n",
    "rcm_clicks = RandomClickModel().assignClicks(ranking)\n",
    "\n",
    "print('Ranking: \\t\\t\\t\\t\\t\\t\\t', ranking)\n",
    "print('Clicks generated by Random Click Model: \\t\\t\\t', rcm_clicks)\n",
    "print('Clicks generated by Simple Dependent Click Model: \\t\\t', sdcm_clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter 𝑎uq. Use the relevance label to assign this parameter by setting 𝑎uq for a document u in the ranked list accordingly. (See Click Models for Web Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_experiment(ranked_pairs, N, click_model):\n",
    "    '''\n",
    "    Simulate an on-line user experiment to compare the ranking performance\n",
    "    of two algorithms, P and E.\n",
    "    \n",
    "    Args:\n",
    "        interleaved_rankings (list): A list containing interleaved rankings and their origins\n",
    "        N (int): The number of simulations to run per click model.\n",
    "        click_model (object): An object representing a click model able to assign clicks to a ranking.\n",
    "        \n",
    "    Returns:\n",
    "        list: The proportion p of wins for algorithm E (1) over P (0) per interleaving\n",
    "    '''\n",
    "    \n",
    "    # Obtain interleaved rankings\n",
    "    interleaved_rankings = []\n",
    "    for ranking_A, ranking_B in ranked_pairs:\n",
    "        interleaved_rankings.append(interleave(ranking_A, ranking_B))\n",
    "    \n",
    "    # Simulate N experiments\n",
    "    win_ratios = []\n",
    "    for interleaved_ranking in interleaved_rankings:\n",
    "        # Obtain ranking and the originating algorithm\n",
    "        ranking = interleaved_ranking['ranking']\n",
    "        origins = interleaved_ranking['origins']\n",
    "        \n",
    "        # Initialize clicks\n",
    "        clicks_total = 0\n",
    "        clicks_E = 0\n",
    "        for i in range(N):\n",
    "            # Simulate clicks\n",
    "            clicks = click_model.assignClicks(ranking)\n",
    "\n",
    "            # Store number of clicks for E as well as total number of clicks.\n",
    "            for click, origin in zip(clicks, origins):\n",
    "                clicks_total += click\n",
    "                if click + origin == 2:\n",
    "                    clicks_E += 1\n",
    "        win_ratio = clicks_E/float(clicks_total) if clicks_total > 0 else 0\n",
    "        win_ratios.append(win_ratio)\n",
    "    return win_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499384731868\n"
     ]
    }
   ],
   "source": [
    "# Obtain ranked pairs from algorithm P and E\n",
    "relevance_labels = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "ranked_pairs = generate_ranking_pairs(relevance_labels, rank_len)\n",
    "\n",
    "# Specify simulation parameters\n",
    "N = 10\n",
    "click_log_path = 'YandexRelPredChallenge.txt'\n",
    "click_model = SdcmClickModel(click_log_path)\n",
    "\n",
    "# Compute win ratio of algorithm E (1) over P (0)\n",
    "win_ratio = simulate_experiment(ranked_pairs, N, click_model)\n",
    "import numpy as np\n",
    "print(np.mean(np.array(win_ratio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
