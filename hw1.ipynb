{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "## 1) Hypothesis Testing – The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this: <br>\n",
    "A. Modify/Build an algorithm<br>\n",
    "B. Compare the algorithm to a baseline by running a hypothesis test.<br>\n",
    "C. If not significant, go back to step A<br>\n",
    "D. If significant, start writing a paper. <br>\n",
    "<br>\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = α):<br>\n",
    "<br>\n",
    "a) P(mth experiment gives significant result | m experiments lacking power to reject H0)?\n",
    "\n",
    "b) P(at least one significant result | m experiments lacking power to reject H0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a) If all m experiments lack the power reject the null hypothesis, we still assume that the results come from the probability distribution of H0. The probability of a significant result is given by the probability mass to the right of the critical value, corrected as the family-wise error rate.\n",
    " \n",
    "P(mth experiment gives significant result | m experiments lacking power to reject H0) = $$1 - (1-\\alpha)^m$$\n",
    "\n",
    "b) The probability of finding at least one significant result is the complement of finding no significant results.\n",
    "\n",
    "P(at least one significant result | m experiments lacking power to reject H0) =\n",
    "\n",
    "$$1 - (1-\\alpha)^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bias and unfairness in Interleaving experiments [10 points]\n",
    "\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ⅔ of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Geen idee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical part\n",
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "The function ```generate_ranking_pairs``` defined below is used to generate all possible ranking pairs of a predefined length for a given set of relevance labels, which will be used later to compare offline and online evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible ranking pairs:  58806\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    " \n",
    "def generate_ranking_pairs(grades, rank_len):\n",
    "    \"\"\" Generates all possible ranking pairs, excluding pairs of \n",
    "        equal rankings\n",
    "\n",
    "    Args:\n",
    "        grades (list): Possible relevance labels in ranking.\n",
    "        rank_len (int): Length of ranking pairs.\n",
    "\n",
    "    Returns:\n",
    "        generator: All possible ranking pairs.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # generate all possible rankings\n",
    "    rankings = product(grades, repeat=rank_len)\n",
    "    # generate all possible pairs of rankings\n",
    "    pairs = product(rankings, repeat=2)    \n",
    "    # exclude pairs of equal rankings\n",
    "    pairs = filter(lambda pair: pair[0] != pair[1], pairs)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "relevance_labels = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "\n",
    "ranking_pairs = list(generate_ranking_pairs(relevance_labels, rank_len))\n",
    "print('Total number of possible ranking pairs: ', len(ranking_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "In this section, several offline evaluation measures are implemented. The first one, average precision, is a simple binary evaluation method, which is calculated as the average of precisions at relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_precision(rankings):    \n",
    "    \"\"\"Calculates Average Precision (AP) = Average of precisions at relevant documents\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The average precision of rankings.\n",
    "        \n",
    "    \"\"\"\n",
    "    relevant = 0\n",
    "    numerator = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        if rel == 'R' or rel == 'HR':\n",
    "            relevant += 1\n",
    "            numerator += relevant/rank\n",
    "    return numerator/len(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to some multi-graded evaluation measures:\n",
    "\n",
    "The scores for relevance labels that are used are 0, 1 and 5 for N, R and HR respectively (equal to example in slides).\n",
    "\n",
    "The nDCG@k measures requires the total number of relevant and highly relevant documents in the entire collection. Since\n",
    "for this dummy example there does not really exist a corpus, another approach is required:\n",
    "\n",
    "The ```nDCGk``` function is fed a ranking of length 5 (one of the permutations created in Step 1). This list of five is treated\n",
    "as the corpus. The list consisting of the first k elements is seen as the result of a query q, and the DCGk of this\n",
    "list is calculated by the ```DCGk``` function. To find the perfect ranking (for normalization), the corpus list is sorted (descending)\n",
    "and the DCGk of the top k elements of the resulting list is calculated. The result is then used for normalization \n",
    "in ```nDCGk```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def DCGk(rankings):\n",
    "    \"\"\"Calculates Discounted Cumulative Gain (DCG)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "\n",
    "    Returns:\n",
    "        float: The discounted cumulative gain.\n",
    "    \"\"\"\n",
    "    discounted_gain = 0\n",
    "    for rank, rel in enumerate(rankings):\n",
    "        rank += 1\n",
    "        gain = (2**rel)-1\n",
    "        discount = 1/math.log(rank+1,2)    \n",
    "        discounted_gain += gain*discount        \n",
    "    return discounted_gain\n",
    "\n",
    "def nDCGk(rankings, k=3):\n",
    "    \"\"\"Calculates Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query. (treated as corpus)\n",
    "        k (int): rank k. Value of 3 is used if no argument is given.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized discounted gain at rank k.\n",
    "    \"\"\"\n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5 if x is 'HR' else 1 if x is 'R' else 0 for x in rankings]\n",
    "    # calculate discounted gain for top k\n",
    "    DCG = DCGk(rankings[:k])\n",
    "    # sort all relevant documents (descending) in the corpus by their \n",
    "    # relative relevance to find best possible DCG result (perfect ranking)\n",
    "    perfect_DCG = DCGk(sorted(rankings, reverse=True)[:k])\n",
    "    # normalize discounted_gain by this result\n",
    "    return DCG/perfect_DCG\n",
    "\n",
    "#nDCGk(ranking_pairs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected reciprocal rank (ERR) metric supports graded relevance judgments and assumes a cascade browsing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ERR(rankings, k=5):\n",
    "    \"\"\"Computes the Expected Reciprocal Rank (ERR) metric in linear time. Based on paper by Chapelle et al.\n",
    "\n",
    "    Args:\n",
    "        rankings (list): ranked result of query.\n",
    "        k (int): rank k. Value of 5 is used if no argument is given.\n",
    "\n",
    "    Returns:\n",
    "        float: the Expected Reciprocal Rank (ERR).\n",
    "        \n",
    "    \"\"\"\n",
    "    # translate relevance to corresponding score for calculations\n",
    "    rankings = [5 if x is 'HR' else 1 if x is 'R' else 0 for x in rankings]\n",
    "    p = 1.0\n",
    "    err = 0.0\n",
    "    for rank, rel in enumerate(rankings[:k]):\n",
    "        rank += 1\n",
    "        R = ((2**rel)-1) / (2**max(rankings))\n",
    "        err += p*(R/rank)\n",
    "        p *= 1-R\n",
    "    return err\n",
    "\n",
    "# ERR(ranking_pairs[100][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the delta measure (0 points)\n",
    "Using the offline evaluation measures we implemented above, we can quantify the difference between ranking pairs as the \"delta measure\". We'll apply this measure to the ranking pairs generated in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_measure(P, E, measure):\n",
    "    \"\"\"Computes the delta of P and E for average precision, nDCG@k or ERR.\n",
    "\n",
    "    Args:\n",
    "        P (list): ranked result of production algorithm.\n",
    "        E (list): ranked result of experimental algorithm.\n",
    "        measure (string): measure to be calculated (average precision, nDCGk, ERR)\n",
    "\n",
    "    Returns:\n",
    "        float: delta of E and P. If < 0, E does not outperform P.\n",
    "    \"\"\"\n",
    "    if measure == 'average precision':\n",
    "        P_measure = average_precision(P)\n",
    "        E_measure = average_precision(E)\n",
    "    elif measure == 'nDCGk':\n",
    "        P_measure = nDCGk(P)\n",
    "        E_measure = nDCGk(E)\n",
    "    elif measure == 'ERR':\n",
    "        P_measure = ERR(P)\n",
    "        E_measure = ERR(E)\n",
    "    \n",
    "    return E_measure - P_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8c46954d9b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m delta_ranking_ap = [delta_measure(pair[0], pair[1], 'average precision') for pair in ranking_pairs \n\u001b[1;32m      7\u001b[0m                     if delta_measure(pair[0], pair[1], 'average precision') > 0]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdelta_ranking_ndcgk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdelta_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nDCGk'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranking_pairs\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#                     if delta_measure(pair[0], pair[1], 'nDCGk') > 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m delta_ranking_err = [delta_measure(pair[0], pair[1], 'ERR') for pair in ranking_pairs \n",
      "\u001b[0;32m<ipython-input-20-8c46954d9b6e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m delta_ranking_ap = [delta_measure(pair[0], pair[1], 'average precision') for pair in ranking_pairs \n\u001b[1;32m      7\u001b[0m                     if delta_measure(pair[0], pair[1], 'average precision') > 0]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdelta_ranking_ndcgk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdelta_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nDCGk'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranking_pairs\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#                     if delta_measure(pair[0], pair[1], 'nDCGk') > 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m delta_ranking_err = [delta_measure(pair[0], pair[1], 'ERR') for pair in ranking_pairs \n",
      "\u001b[0;32m<ipython-input-10-c860daf26564>\u001b[0m in \u001b[0;36mdelta_measure\u001b[0;34m(P, E, measure)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nDCGk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mP_measure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnDCGk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mE_measure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnDCGk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ERR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mP_measure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mERR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8240af6d91f4>\u001b[0m in \u001b[0;36mnDCGk\u001b[0;34m(rankings, k)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mperfect_DCG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# normalize discounted_gain by this result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDCG\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mperfect_DCG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#nDCGk(ranking_pairs[0][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "relevance_labels = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "\n",
    "ranking_pairs = list(generate_ranking_pairs(relevance_labels, rank_len))\n",
    "\n",
    "delta_ranking_ap = [delta_measure(pair[0], pair[1], 'average precision') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'average precision') > 0]\n",
    "delta_ranking_ndcgk = [delta_measure(pair[0], pair[1], 'nDCGk') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'nDCGk') > 0]\n",
    "delta_ranking_err = [delta_measure(pair[0], pair[1], 'ERR') for pair in ranking_pairs \n",
    "                    if delta_measure(pair[0], pair[1], 'ERR') > 0]\n",
    "\n",
    "print('Using AP, E outperforms P in {} out of {} times.'.format(len(delta_ranking_ap), len(ranking_pairs)))\n",
    "# print('Using nDCGk, E outperforms P in {} out of {} times.'.format(len(delta_ranking_ndcgk), len(ranking_pairs)))\n",
    "print('Using ERR, E outperforms P in {} out of {} times.'.format(len(delta_ranking_err), len(ranking_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def interleave(ranking_A, ranking_B):\n",
    "    \"\"\"\n",
    "    Interleaves rankings from two different ranking algorithms into \n",
    "    one ranking based on Team Draft Interleaving. Both rankings are\n",
    "    assumed to have equal length.\n",
    "    \n",
    "    Args:\n",
    "        ranking_A (list): Ranking of algorithm A.\n",
    "        ranking_B (list): Ranking of algorithm B.\n",
    "        \n",
    "    Returns:\n",
    "        (Dict): Dict containing two lists: the interleaved ranking and the origins.\n",
    "    \"\"\"\n",
    "    ranking_I = []\n",
    "    origins = []\n",
    "    i = 0\n",
    "    while len(ranking_I) < len(ranking_A):\n",
    "        # A wins\n",
    "        if randint(0,1) == 0:\n",
    "            origins += [0,1]\n",
    "            ranking_I.append(ranking_A[i])\n",
    "            ranking_I.append(ranking_B[i])\n",
    "        # B wins\n",
    "        else:\n",
    "            origins += [1,0]\n",
    "            ranking_I.append(ranking_B[i])\n",
    "            ranking_I.append(ranking_A[i])\n",
    "        i += 1\n",
    "    if len(ranking_I) > len(ranking_A):\n",
    "        ranking_I = ranking_I[0:-1]\n",
    "        origins = origins[0:-1]\n",
    "    return {'ranking': ranking_I, 'origins': origins}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'R', 'HR', 'N'] ['R', 'HR', 'N', 'N', 'R']\n",
      "['R', 'N', 'N', 'HR', 'N']\n"
     ]
    }
   ],
   "source": [
    "ranking_A = ['N','N','R','HR','N']\n",
    "ranking_B = ['R','HR','N','N','R']\n",
    "interleaved_ranking = interleave(ranking_A, ranking_B)\n",
    "print(ranking_A, ranking_B)\n",
    "print(interleaved_ranking['ranking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement User Clicks Simulation (15 points)\n",
    "### Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "class RandomClickModel:\n",
    "    '''\n",
    "    Implements a Random Click Model. This model decides to click on a document\n",
    "    with a probability P_click without taking anything else into account. P_click\n",
    "    is learned from a click log. The default click log is from Yandex.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, click_log_path = 'YandexRelPredChallenge.txt'):\n",
    "        self.P_click = self.train(click_log_path)\n",
    "        \n",
    "    def train(self, click_log_path):\n",
    "        '''\n",
    "        Estimates the only parameter P_click of the Random Click Model using a click log\n",
    "        by dividing the total amount of clicks by the total amount of shown documents.\n",
    "\n",
    "        Args:\n",
    "            click_log_path (String): Location of the click log.\n",
    "\n",
    "        Return:\n",
    "            Float: The P_click parameter used to decide whether to click on a document.\n",
    "\n",
    "        '''\n",
    "        shown_docs = 0\n",
    "        clicks = 0\n",
    "        with open(click_log_path,'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "\n",
    "                # Count all shown docs\n",
    "                if line[2] == 'Q':\n",
    "                    shown_docs += len(line)-5\n",
    "\n",
    "                # Count all clicks\n",
    "                else:\n",
    "                    clicks += 1\n",
    "        P_click = clicks/float(shown_docs)\n",
    "        return P_click\n",
    "\n",
    "    def predictProb(self, ranking):\n",
    "        '''\n",
    "        Generates the probability for each document in a ranking to be clicked on\n",
    "        based on a Random Click Model.\n",
    "\n",
    "        Args:\n",
    "            ranking (List): List of ranked documents represented by relevance.\n",
    "\n",
    "        Return:\n",
    "            List: A list of click probabilities for each document in the ranking.\n",
    "        '''\n",
    "        click_probabilities = []\n",
    "        for doc in ranking:\n",
    "            click_probabilities.append(self.P_click)\n",
    "        return click_probabilities\n",
    "\n",
    "    def assignClicks(self, ranking):\n",
    "        '''\n",
    "        Based on their click probabilities, either do or do not assign a click to each document.\n",
    "\n",
    "        Args:\n",
    "            ranking_probabilities (List): A list of click probability and document tuples.\n",
    "            P_click (float): Probability used to decide whether to click on a document.\n",
    "\n",
    "        Return:\n",
    "            List: A list representing clicks with 1's on documents in ranking with the same index.\n",
    "        '''\n",
    "        click_probabilities = self.predictProb(ranking)\n",
    "        clicks = []\n",
    "        for prob in click_probabilities:\n",
    "            if uniform(0,1) < prob:\n",
    "                clicks.append(1)\n",
    "            else:\n",
    "                clicks.append(0)\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ranking_I = ['HR','R','HR','N','R']\n",
    "clicks = RandomClickModel().assignClicks(ranking_I)\n",
    "print(clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dependent Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class SdcmClickModel(object):\n",
    "    \"\"\" Implements a Simple Dependent Click Model (SDCM). Generates clicks starting\n",
    "        from the first rank, and stochastically decides if a clicked result is \n",
    "        satisfactory, using probabilities trained on a click log. If the result was \n",
    "        not satisfactory, the next result is examined, and then clicked with a \n",
    "        probability dependent on the corresponding relevance label. The probabilities\n",
    "        for each of the relevance labels are taken from the book \"Click Models for \n",
    "        Web Search\".\n",
    "    \n",
    "    Args:\n",
    "        click_log_path (str): Location of the click log.\n",
    "        attr_model (str): Model used for attractiveness parameters\n",
    "                            (perfect/navigational/informative)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Click model attractiveness parameters\"\"\"\n",
    "    attr_perf = {'HR':1.0, 'R':0.5, 'N':0.0}\n",
    "    attr_nav = {'HR':0.95, 'R':0.5, 'N':0.05}\n",
    "    attr_inf = {'HR':0.9, 'R':0.7, 'N':0.4} \n",
    "    \n",
    "    def __init__(self, click_log_path='YandexRelPredChallenge.txt', attr_model='nav'):\n",
    "        self.click_log_path = click_log_path\n",
    "        if attr_model == 'nav':\n",
    "            self.attr_model = SdcmClickModel.attr_nav\n",
    "        elif attr_model == 'perf':\n",
    "            self.attr_model = SdcmClickModel.attr_perf            \n",
    "        elif attr_model == 'inf':\n",
    "            self.attr_model = SdcmClickModel.attr_inf\n",
    "        else:\n",
    "            raise ValueError('The attractiveness model \"{}\" is not available.' \\\n",
    "                            ' Choose from \"nav\", \"perf\" or \"inf\".'.format(attr_model))\n",
    "        self.params = self.learnParams()\n",
    "            \n",
    "    def learnParams(self):\n",
    "        \"\"\" Learns parameters for simple dependent click model.\n",
    "\n",
    "        Returns:\n",
    "            dict: Learnt click model parameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with open(self.click_log_path,'r') as f:\n",
    "            # global rank click counter\n",
    "            rank_counter = Counter()\n",
    "            # counter for clicks on consecutive ranks\n",
    "            pair_counter = Counter()\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                time_passed = line[1]\n",
    "                if time_passed == '0':\n",
    "                    prev_clicked_rank = -100\n",
    "                if line[2] == 'Q':\n",
    "                    results = line[5:]\n",
    "                else:\n",
    "                    url = line[3]\n",
    "                    try:\n",
    "                        clicked_rank = results.index(url)\n",
    "                    except: \n",
    "                        pass\n",
    "                    rank_counter[clicked_rank] += 1\n",
    "                    if clicked_rank == prev_clicked_rank + 1:\n",
    "                        pair_counter[prev_clicked_rank] += 1\n",
    "                    prev_clicked_rank = clicked_rank\n",
    "        lambdas = {r+1:pair_counter[r]/rank_counter[r] for r in pair_counter.keys()}\n",
    "        return lambdas\n",
    "\n",
    "    \n",
    "    def attrProbs(self, ranking):\n",
    "        \"\"\" Assigns attractiveness probabilities for list of relevance labels.\n",
    "\n",
    "        Args:\n",
    "            ranking (list): Ranked list of relevance labels.\n",
    "\n",
    "        Returns:\n",
    "            list: List of attractiveness probabilities corresponding with ranking.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = [self.attr_model[label] for i, label in enumerate(ranking)]\n",
    "        return probs\n",
    "\n",
    "    def assignClicks(self, ranking):\n",
    "        \"\"\" Assigns clicks based on attractiveness and satisfactoriness probabilities.\n",
    "\n",
    "        Args:\n",
    "            ranking (list): Ranked list of relevance labels.\n",
    "\n",
    "        Returns:\n",
    "            list: Assigned clicks.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = self.attrProbs(ranking)\n",
    "        \n",
    "        clicks = []  \n",
    "        for i in range(len(probs)):\n",
    "            if i == 0:\n",
    "                # for first result, click probability equals attractiveness\n",
    "                p_click = probs[i]\n",
    "            else:\n",
    "                prev_click = clicks[i-1]\n",
    "                if prev_click == 1:\n",
    "                    p_satisf = 1 - self.params[i]\n",
    "                    # stochastically decide satisfactoriness    \n",
    "                    outcome = random()\n",
    "                    if outcome < p_satisf:\n",
    "                        # when the clicked result was satisfactory, stop\n",
    "                        clicks += ([0] * (len(probs) - len(clicks)))\n",
    "                        break\n",
    "                    else:\n",
    "                        p_click = probs[i]\n",
    "                else:\n",
    "                    p_click = probs[i]\n",
    "            # stochastically decide click \n",
    "            outcome = random()\n",
    "            if outcome < p_click:\n",
    "                clicks.append(1)\n",
    "            else: clicks.append(0)\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ranking = ['HR', 'R', 'R', 'N', 'HR', 'N', 'R']\n",
    "clicks = SdcmClickModel().assignClicks(ranking)\n",
    "print(clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter 𝑎uq. Use the relevance label to assign this parameter by setting 𝑎uq for a document u in the ranked list accordingly. (See Click Models for Web Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_experiment(ranked_pairs, N, click_model):\n",
    "    '''\n",
    "    Simulate an on-line user experiment to compare the ranking performance\n",
    "    of two algorithms, P and E.\n",
    "    \n",
    "    Args:\n",
    "        interleaved_rankings (List): A list containing interleaved rankings and their origins\n",
    "        N (int): The number of simulations to run per click model.\n",
    "        click_model (object): An object representing a click model able to assign clicks to a ranking.\n",
    "        \n",
    "    Returns:\n",
    "        float: The proportion p of wins for algorithm E (1) over P (0)\n",
    "    '''\n",
    "    \n",
    "    # Obtain interleaved rankings\n",
    "    interleaved_rankings = []\n",
    "    for ranking_A, ranking_B in ranked_pairs:\n",
    "        interleaved_rankings.append(interleave(ranking_A, ranking_B))\n",
    "    \n",
    "    # Simulate N experiments\n",
    "    clicks_total = 0\n",
    "    clicks_E = 0\n",
    "    for i in range(N):\n",
    "        for interleaved_ranking in interleaved_rankings:\n",
    "            ranking = interleaved_ranking['ranking']\n",
    "            origins = interleaved_ranking['origins']\n",
    "    \n",
    "            # Simulate clicks\n",
    "            clicks = click_model.assignClicks(ranking)\n",
    "            \n",
    "            # Store number of clicks for E as well as total number of clicks.\n",
    "            for click, origin in zip(clicks,origins):\n",
    "                clicks_total += click\n",
    "                if click + origin == 2:\n",
    "                    clicks_E += 1\n",
    "    return clicks_E/float(clicks_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5009018766590962\n"
     ]
    }
   ],
   "source": [
    "# Obtain ranked pairs from algorithm P and E\n",
    "grades = ['HR', 'R', 'N']\n",
    "rank_len = 5\n",
    "ranked_pairs = generate_ranking_pairs(grades, rank_len)\n",
    "\n",
    "# Specify simulation parameters\n",
    "N = 10\n",
    "click_log_path = 'YandexRelPredChallenge.txt'\n",
    "click_model = RandomClickModel(click_log_path)\n",
    "\n",
    "# Compute win ratio of algorithm E (1) over P (0)\n",
    "win_ratio = simulate_experiment(ranked_pairs, N, click_model)\n",
    "print(win_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
