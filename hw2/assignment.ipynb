{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.5 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.5, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "#print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "#print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    topics = parse_topics([f_topics])\n",
    "    #print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ð›Œ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ð› [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ð›… in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of â€œsoftâ€ passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ð›” equal to 50, and Dirichlet smoothing with ð› optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Donâ€™t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "docs_unigram = collections.defaultdict(dict)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        #term frequency\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        # document frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "        # unigram model (added)\n",
    "        docs_unigram[int_doc_id][query_term_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def tfidfScore(doc_id, query):\n",
    "    \"\"\"\n",
    "    TF-IDF scoring function for a document and a query term\n",
    "    \n",
    "    :param doc_id: the document id\n",
    "    :param query: the query (list of query term id's)\n",
    "    \"\"\"\n",
    "    tfidf_score = 0.0\n",
    "    for t in query:\n",
    "        try:\n",
    "            # tf = how often does term appear in document\n",
    "            tf = inverted_index[t][doc_id]\n",
    "            idf = log(num_documents) - log(len(inverted_index[t]))\n",
    "            tfidf_score += log(1+tf)*idf\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BM25Score(doc_id, query, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 scoring function for a document and a query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query: the query (list of terms)\n",
    "    :param k1: (fixed at 1.2)\n",
    "    :param b: (fixed at 0.75)\n",
    "    \"\"\"\n",
    "    bm25 = 0.0\n",
    "    for q in set(query):\n",
    "        try:\n",
    "            tf = inverted_index[q][doc_id]\n",
    "            w = ((k1+1)*tf) / (k1*((1-b)+b*(avg_doc_length))+tf)\n",
    "            idf = log(num_documents) - log(len(inverted_index[q]))\n",
    "            # use logs to prevent underflow\n",
    "            bm25 += w + idf \n",
    "            \n",
    "        \n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-45.500192454108415\n"
     ]
    }
   ],
   "source": [
    "def constructLM(document_term_freqs, collection_term_freqs, smoothing, param):\n",
    "    '''\n",
    "    Constructs language model using several different smoothing techniques\n",
    "    \n",
    "    Jelinek-Mercer smoothing linearly interpolates using the background language model.\n",
    "    Intuively, this captures the assumption that for an unseen word, its smoothed\n",
    "    probability should depend on the frequency of its appearence over all documents. \n",
    "    LM is a multinomial distrubution (unigram). Param=0 uses only the background LM\n",
    "    \n",
    "    Dirichlet smoothing computes doc-term probabilities using both the document LM\n",
    "    as well as the background LM, and does so while taking into account\n",
    "    the document's length. The intuition behind this is that longer\n",
    "    documents need less smoothing because they can rely more on their own LM.\n",
    "    \n",
    "    Absolute discounting substracts param from each document term frequency count\n",
    "    and adds it to unseen words, thus redistributes the probability mass. \n",
    "    \n",
    "    :param document_term_freq: the document term frequencies \n",
    "    :param collection_term_freq: the collection term frequencies used for background LM\n",
    "    :param smoothing: different types of smoothing\n",
    "    :param param: decides the degree of smoothing\n",
    "    '''\n",
    "    n_doc_tokens = sum(document_term_freqs.values())\n",
    "    n_doc_types = sum(set(document_term_freqs.values()))\n",
    "    n_col_tokens = sum(collection_term_freqs.values())\n",
    "\n",
    "    document_term_freqs = collections.defaultdict(int,document_term_freqs)\n",
    "    doc_term_prob = collections.defaultdict(float)\n",
    "    \n",
    "    # Loop over vocabulary (query terms) and construct JM-smoothed document language model\n",
    "    for q_t_id in query_term_ids:\n",
    "        doc_tf = document_term_freqs[q_t_id]\n",
    "        col_tf = collection_term_freqs[q_t_id]\n",
    "        \n",
    "        if smoothing == 'JM':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = param*(doc_tf/float(n_doc_tokens)) + (1-param)*(col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "                \n",
    "        elif smoothing == 'dirichlet':\n",
    "            #doc_term_prob[q_t_id] = (doc_tf + (param*col_tf))/(n_doc_tokens+param)\n",
    "            doc_term_prob[q_t_id] = (doc_tf + param*(col_tf/n_col_tokens)) / (n_doc_tokens+param)\n",
    "            \n",
    "        elif smoothing == 'discount':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = max(doc_tf-param,0)/n_doc_tokens + (param*n_doc_types)/n_doc_tokens * (col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "        else:\n",
    "            raise('Smoothing method not recognized!')\n",
    "\n",
    "            \n",
    "    return doc_term_prob\n",
    "\n",
    "def queryLikelihood(query_id, LM):\n",
    "    '''\n",
    "    Calculates query likelihood given language model and query\n",
    "    \n",
    "    :param query_id: query id\n",
    "    :param LM language model\n",
    "    '''\n",
    "    if LM is None:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    q_likelihood = 0\n",
    "    for q_t_id in tokenized_queries[query_id]:\n",
    "        try:\n",
    "            q_likelihood += log(LM[q_t_id])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return q_likelihood\n",
    "\n",
    "\n",
    "doc_id = index.document_base()\n",
    "q_id = '53'\n",
    "doc_t_f = inverted_index[1]\n",
    "smoothing = 'dirichlet'\n",
    "param = 0.1\n",
    "\n",
    "LM = constructLM(doc_t_f, collection_frequencies, smoothing, param)\n",
    "print(queryLikelihood(q_id, LM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def gaussian(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Gaussian kernel function\"\"\"\n",
    "    return math.exp(-(i-j)**2/(2*sigm**2))\n",
    "\n",
    "def triangle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Triangle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1 - abs(i-j)/sigm\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def cosine(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Cosine (Hamming) kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 0.5*(1 + math.cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value    \n",
    "\n",
    "def circle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Circle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = (1 + cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value     \n",
    "    \n",
    "def passage(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Passage kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def PLM_prob(int_document_id, query_term_id, i, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates estimated language model of virtual document\"\"\"\n",
    "    _, doc_token_ids = index.document(int_document_id)\n",
    "    ocs = [i+1 for i, x in enumerate(doc_token_ids) if x == query_term_id] \n",
    "    prop_count = sum([kernel(i,j) for j in ocs])\n",
    "    N = document_lengths[int_document_id]\n",
    "#     z_i = sum([kernel(i,j) for j in range(1,N+1)])\n",
    "    sigma = 50\n",
    "    if kernel == gaussian:\n",
    "        z_i = math.sqrt(2*math.pi*sigma**2)*(norm.cdf((N-i)/float(sigma) -\n",
    "                                                      norm.cdf((1-i)/sigma)))\n",
    "    elif kernel == triangle:\n",
    "        z_i = sigma\n",
    "    elif kernel == cosine:\n",
    "        z_i = sigma\n",
    "    elif kernel == circle:\n",
    "        raise NotImplementedError\n",
    "    elif kernel == passage:\n",
    "        z_i = 2*sigma\n",
    "    # smoothed using dirichlet\n",
    "    col_lang_model = collection_frequencies[query_term_id\n",
    "                                           ]/sum(collection_frequencies.values())\n",
    "    prob = (prop_count + mu * col_lang_model/(float(z_i) + mu))\n",
    "    return prob\n",
    "\n",
    "def PLM_score(int_document_id, query, i, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates score for PLM at position i\"\"\"\n",
    "    score = 0\n",
    "    query_len = len(query)\n",
    "    for query_term_id in query:\n",
    "        #ML Estimate of query language model\n",
    "        ocs = [True for x in query if x == query_term_id] \n",
    "        query_lan_model = len(ocs)/float(query_len)\n",
    "        score += query_lan_model * math.log(query_lan_model/PLM_prob(int_document_id, \n",
    "                                                                     query_term_id, \n",
    "                                                                     i, mu=mu,\n",
    "                                                                     kernel=kernel))\n",
    "    return -1 * score\n",
    "            \n",
    "def PLM(int_document_id, query, mu=50,kernel=gaussian):\n",
    "    \"\"\"Scores document using PLM Best Position Strategy\"\"\"\n",
    "    N = document_lengths[int_document_id]\n",
    "    best_score = 0\n",
    "    for i in range(1,N+1):\n",
    "        score = PLM_score(int_document_id, query, i, mu=mu, kernel=kernel)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, param=0.1):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name+str(param))\n",
    "\n",
    "    if model_name == 'lsi' or model_name == 'lsa':\n",
    "        print('creating LSM...')\n",
    "        lsm = LSM(model_name)\n",
    "        print('created LSM.')\n",
    "    \n",
    "    print('Retrieving using', model_name,'...')\n",
    "    \n",
    "    data = collections.defaultdict(list)    \n",
    "    i=25000\n",
    "    for doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        if doc_id % i == 0: print(\"Percentage completed: %.2f\" % (doc_id/index.maximum_document()))\n",
    "        \n",
    "        if model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "            LM = constructLM(docs_unigram[doc_id], collection_frequencies, model_name, param)\n",
    "         \n",
    "        for query_id, query_terms in tokenized_queries.items():\n",
    "            if model_name == 'tfidf' or model_name == 'BM25':\n",
    "                score = score_fn(doc_id, tokenized_queries[query_id])\n",
    "\n",
    "            # smoothing\n",
    "            elif model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "                score = score_fn(query_id, LM)\n",
    "            \n",
    "            # PLM\n",
    "            elif model_name == 'PLM':\n",
    "                calculate_score = False\n",
    "                for q in tokenized_queries[query_id]:\n",
    "                    if q in set(index.document(doc_id)[1]):\n",
    "                        calculate_score = True\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                # TO DO functie Alex: voordat berekenen check in welke documenten geen query term zit --> None / 0\n",
    "                if calculate_score == True:\n",
    "                    score = score_fn(doc_id, tokenized_queries[query_id], mu=param)\n",
    "                else:\n",
    "                    score = None\n",
    "                \n",
    "            #use external document id (index.document(doc_ID))\n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "    \n",
    "    print('Retrieving completed.')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TO DO vrijdag\n",
    "# - meer optimalisatie parameters (voor in plots)\n",
    "# - plots in verslag\n",
    "# - resultaten Alex\n",
    "# - t-test in verslag (na optimalisatie parametes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating LSM...\n",
      "created LSM.\n",
      "Retrieving using lsi ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('lsi', LSM_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.0102',\n",
       " 'P5': '0.0333',\n",
       " 'nDCG': '0.0337',\n",
       " 'recall': 0.20902429618202853}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'lsi.run',mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trec_eval(qrel, run, mean=True):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    if mean==True:\n",
    "        # obtain nDCG@10\n",
    "        nDCG = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^ndcg\"\n",
    "        scores['nDCG'] = nDCG[-8].split()[-1]\n",
    "        # obtain MAP@1000\n",
    "        MAP = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^map\\s\"\n",
    "        scores['MAP'] = MAP[-1].split()[-1]\n",
    "        # obtain Precision@5\n",
    "        P5 = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^P_5\\s\"\n",
    "        scores['P5'] = P5[-1].split()[-1]\n",
    "        # obtain recall\n",
    "        stats = !trec_eval/./trec_eval {qrel} {run}\n",
    "        num_rel_ret = stats[4].split()[-1]\n",
    "        num_rel = stats[3].split()[-1]    \n",
    "        scores['recall'] = float(num_rel_ret)/float(num_rel)\n",
    "    \n",
    "    else:\n",
    "        nDCG = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^ndcg_cut_10 \"\n",
    "        scores['nDCG'] = np.array([float(s.split()[-1]) for s in nDCG[:-1]])\n",
    "\n",
    "        MAP = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^map\\s\"            \n",
    "        scores['MAP'] = np.array([float(s.split()[-1]) for s in MAP[:-1]])\n",
    "\n",
    "        P5 = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^P_5\\s\"\n",
    "        scores['P5'] = np.array([float(s.split()[-1]) for s in P5[:-1]])\n",
    "\n",
    "        recall = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^recall_1000\\s\"\n",
    "        scores['recall'] = np.array([float(s.split()[-1]) for s in recall[:-1]])\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3933333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_scores = trec_eval('ap_88_89/qrel_test', 'discount0.9.run',mean=False)\n",
    "np.mean(discount_scores['P5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using tfidf ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('tfidf', tfidfScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.2195',\n",
       " 'P5': '0.3467',\n",
       " 'nDCG': '0.3586',\n",
       " 'recall': 0.6239876590821443}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'tfidf.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using BM25 ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('BM25', BM25Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.0978',\n",
       " 'P5': '0.1200',\n",
       " 'nDCG': '0.1157',\n",
       " 'recall': 0.43540300809872734}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'BM25.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using JM ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('JM', queryLikelihood, param=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using JM ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('JM', queryLikelihood, param=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using JM ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('JM', queryLikelihood, param=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.2025',\n",
       " 'P5': '0.3600',\n",
       " 'nDCG': '0.3571',\n",
       " 'recall': 0.5769379097570382}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'JM0.1.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using dirichlet ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "# [500, 1000, 1500]\n",
    "run_retrieval('dirichlet', queryLikelihood, param=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using dirichlet ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('dirichlet', queryLikelihood, param=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.2289',\n",
       " 'P5': '0.3733',\n",
       " 'nDCG': '0.3906',\n",
       " 'recall': 0.6216737369841882}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'dirichlet1000.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using discount ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "# params = [0.1, 0.5, 0.9]\n",
    "run_retrieval('discount', queryLikelihood, param=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using discount ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('discount', queryLikelihood, param=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using discount ...\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.91\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('discount', queryLikelihood, param=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.1893',\n",
       " 'P5': '0.3933',\n",
       " 'nDCG': '0.3881',\n",
       " 'recall': 0.4676166779763251}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_test', 'discount0.9.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using PLM ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-8d6cbe06aaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PLM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-275-e0009f2d248b>\u001b[0m in \u001b[0;36mrun_retrieval\u001b[0;34m(model_name, score_fn, param)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# TO DO functie Alex: voordat berekenen check in welke documenten geen query term zit --> None / 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcalculate_score\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7498fafdca8>\u001b[0m in \u001b[0;36mPLM\u001b[0;34m(int_document_id, query, mu, kernel)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLM_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7498fafdca8>\u001b[0m in \u001b[0;36mPLM_score\u001b[0;34m(int_document_id, query, i, mu, kernel)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                                                      \u001b[0mquery_term_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                                                      \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                                                      kernel=kernel))\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7498fafdca8>\u001b[0m in \u001b[0;36mPLM_prob\u001b[0;34m(int_document_id, query_term_id, i, mu, kernel)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"\"\"Calculates estimated language model of virtual document\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_token_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7498fafdca8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"\"\"Calculates estimated language model of virtual document\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_token_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_retrieval('PLM', PLM, param=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_retrieval('PLM', PLM, param=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_retrieval('PLM', PLM, param=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "For the language models, create plots showing NDCG@10 with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's subprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-tailed paired Student t-test\n",
    "Compute significance of the results using a two-tailed paired Student t-test [5 points]. Be wary of false rejection of the null hypothesis caused by the multiple comparisons problem. There are multiple ways to mitigate this problem and it is up to you to choose one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "compare 4 measure results (NDCG@10, MAP@1000, Precision@5, Recall@1000) for different IR methods \n",
    "(tf-idf, bm25, LMs). \n",
    "\n",
    "\n",
    "Solve: multiple comparisons problem:\n",
    "Bonferroni correction\n",
    "alter the alpha. You simply divide .05 by the number of tests that  youâ€™re doing, and go by that. \n",
    "4 tests tests, you look for .05 / 5 = .0125\n",
    "\n",
    "Problem: can be very conservative (i.e., the actual family-wise error rate is much less than the prescribed alpha)\n",
    "e.g. in fMRI analysis, tests are done on over 100,000 voxels in the brain. The Bonferroni method would \n",
    "require p-values to be smaller than .05/100000 to declare significance.\n",
    "In our case (only 4 experiments) this is deemed a non-issue.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    '''\n",
    "    This class creates a generator object from a pyndri index, yielding\n",
    "    a document represented as a bow or dense tfidf compatible with gensim.\n",
    "   \n",
    "    :param: method: which method to use, if lda than return a bow and otherwise\n",
    "            return a dense tfidf. If word2vec than return list of words per doc.\n",
    "    '''\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "            unigram = docs_unigram[int_doc_id]\n",
    "           \n",
    "            # return BOW representation\n",
    "            if self.method == 'lda':\n",
    "                yield list(unigram.items())\n",
    "           \n",
    "            # return TFIDF representation\n",
    "            elif self.method == 'lsi':\n",
    "                yield dense_tfidf(unigram)\n",
    "            elif self.method == 'word2vec':\n",
    "                ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "                yield tuple(dictionary[token_id] for token_id in doc_token_ids \\\n",
    "                            if token_id > 0 and token_id in dictionary)\n",
    "            else:\n",
    "                raise ValueError('Method in MyCorpus undefined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_tfidf(unigram):\n",
    "    '''\n",
    "    Transforms a document with unigram representation to a dense \n",
    "    TFIDF list. Returns a list of (token_id, tfidf) tuples.\n",
    "    '''\n",
    "    tfidf_dict = collections.defaultdict(float)\n",
    "    for token_id, tf in unigram.items():\n",
    "        idf = log(num_documents) - log(len(inverted_index[token_id]))\n",
    "        tfidf_dict[token_id] = log(1+tf)*idf\n",
    "    return list(tfidf_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def LSM(lsm_method, num_topics=10):\n",
    "    '''\n",
    "    Implements two Latent Sematic Models: LSI and LDA using gensim.\n",
    "    \n",
    "    :param method: either lsi or lda. This decides whether the\n",
    "    corpus streaming object returns a BOW or TFIDF representation\n",
    "    and whether the latent semantic model that is being returned\n",
    "    is LSI or LDA.\n",
    "    '''\n",
    "    print('creating LSM with '+str(num_topics)+' topics.')\n",
    "    \n",
    "    corpus = MyCorpus(lsm_method)\n",
    "    if lsm_method == 'lsi':\n",
    "        lsm = LsiModel(corpus, id2word = id2token, num_topics=num_topics)\n",
    "    else:\n",
    "        lsm = LdaModel(list(corpus), id2word = id2token, num_topics=num_topics)\n",
    "    return lsm\n",
    "\n",
    "def LSM_score(query_id, document_id, lsm, lsm_method):\n",
    "    '''\n",
    "    Calculate a query-document score between either LSA \n",
    "    or LDA representations of the query and document\n",
    "    using the cosine similarity.\n",
    "    '''\n",
    "    \n",
    "    # Construct query and and document representation\n",
    "    query_unigram = collections.Counter(\n",
    "        token_id for token_id in tokenized_queries[query_id]\n",
    "        if token_id > 0)\n",
    "    doc_unigram = docs_unigram[document_id]\n",
    "    \n",
    "    if lsm_method == 'lsi':\n",
    "        query_repr = lsm[dense_tfidf(query_unigram)]\n",
    "        doc_repr = lsm[dense_tfidf(doc_unigram)]\n",
    "    else:\n",
    "        query_repr = lsm[list(query_unigram.items())]\n",
    "        doc_repr = lsm[list(doc_unigram.items())]\n",
    "\n",
    "    query_repr = [[topic for idx, topic in query_repr]]\n",
    "    doc_repr = [[topic for idx, topic in doc_repr]]\n",
    "    \n",
    "    try:\n",
    "        score = cosine_similarity(query_repr, doc_repr)\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate document names to document index\n",
    "doc_id_transl = {}\n",
    "for i in range(1,index.document_count()):\n",
    "    doc_id_transl[index.ext_document_id(i)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load top 1000 tf-idf\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('run/tfidf.run') as f:\n",
    "    top_1000 = defaultdict(list)\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        q_id, doc_name = line[0], line[2]\n",
    "        top_1000[q_id].append(doc_id_transl[doc_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSM(top_1000, doc_repres, lsm_method, param, fn):\n",
    "\n",
    "    lsm = LSM(lsm_method, num_topics=param)\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "    for query_id in top_1000.keys():\n",
    "        for doc_id in top_1000[query_id]:\n",
    "            \n",
    "            score = LSM_score(query_id, doc_id, lsm, lsm_method)\n",
    "\n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "\n",
    "    print('Retrieving completed.')\n",
    "\n",
    "    with open(fn, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=lsm_method,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file: run/lda_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=100.run\n",
      "creating LSM with 100 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidstap/anaconda2/envs/ir1/lib/python3.5/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=100.run\n",
      "creating LSM with 100 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "for param in [10, 20, 50, 100, 200]:\n",
    "    print('creating file: run/lda_ntopics='+str(param)+'.run')\n",
    "    run_LSM(top_1000, doc_repres, 'lda', param, 'lda_ntopics='+str(param)+'.run')\n",
    "        print('creating file: run/lsi_ntopics='+str(param)+'.run')\n",
    "    run_LSM(top_1000, doc_repres, 'lsi', param, 'lsi_ntopics='+str(param)+'.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file: run/lsi_ntopics=400.run\n",
      "creating LSM with 400 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=600.run\n",
      "creating LSM with 600 topics.\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "for param in [400,600]:\n",
    "    print('creating file: run/lsi_ntopics='+str(param)+'.run')\n",
    "    run_LSM(top_1000, doc_repres, 'lsi', param, 'lsi_ntopics='+str(param)+'.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file: run/lsi_ntopics=800.run\n",
      "creating LSM with 800 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=1000.run\n",
      "creating LSM with 1000 topics.\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "for param in [800,1000]:\n",
    "    print('creating file: run/lsi_ntopics='+str(param)+'.run')\n",
    "    run_LSM(top_1000, doc_repres, 'lsi', param, 'lsi_ntopics='+str(param)+'.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec_eval('ap_88_898_89/qrel_validation', 'run/lsi_top1000.run',mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSM(model_name, ):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name+str(param))\n",
    "\n",
    "    print('creating LSM...')\n",
    "    lsm = LSM(model_name)\n",
    "    print('created LSM.')\n",
    "    \n",
    "    print('Retrieving using', model_name,'...')\n",
    "    \n",
    "    data = collections.defaultdict(list)    \n",
    "    i=14999\n",
    "    for doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        if doc_id % i == 0: print(\"Percentage completed: %.2f\" % (doc_id/index.maximum_document()))\n",
    "        \n",
    "        if model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "            LM = constructLM(docs_unigram[doc_id], collection_frequencies, model_name, param)\n",
    "        \n",
    "        elif model_name == 'lsi':\n",
    "            doc_unigram = docs_unigram[doc_id]\n",
    "            doc_repr = lsm[dense_tfidf(doc_unigram)]\n",
    "            doc_repr = [[topic for idx, topic in doc_repr]]\n",
    "        elif model_name == 'lda':\n",
    "            doc_unigram = docs_unigram[doc_id]\n",
    "            doc_repr = lsm[list(docs_unigram)]\n",
    "            doc_repr = [[topic for idx, topic in doc_repr]]\n",
    "        \n",
    " \n",
    "        for query_id, query_terms in tokenized_queries.items():\n",
    "            if model_name == 'tfidf' or model_name == 'BM25':\n",
    "                score = score_fn(doc_id, tokenized_queries[query_id])\n",
    "\n",
    "            # smoothing\n",
    "            elif model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "                score = score_fn(query_id, LM)\n",
    "            \n",
    "            # PLM\n",
    "            elif model_name == 'PLM':\n",
    "                score = score_fn(doc_id, tokenized_queries[query_id], mu=param)\n",
    "                \n",
    "            elif model_name == 'lsi' or model_name == 'lsa':\n",
    "                score = score_fn(query_id, doc_id, lsm, model_name, doc_repr)\n",
    "\n",
    "            #use external document id (index.document(doc_ID))\n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "    \n",
    "    print('Retrieving completed.')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
