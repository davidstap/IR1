{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.5 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, _ = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    topics = parse_topics([f_topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ùõå in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ùõç [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ùõÖ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of ‚Äúsoft‚Äù passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ùõî equal to 50, and Dirichlet smoothing with ùõç optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don‚Äôt forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "docs_unigram = collections.defaultdict(dict)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        #term frequency\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        # document frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "        # unigram model (added)\n",
    "        docs_unigram[int_doc_id][query_term_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def tfidfScore(doc_id, query):\n",
    "    \"\"\"\n",
    "    TF-IDF scoring function for a document and a query term\n",
    "    \n",
    "    :param doc_id: the document id\n",
    "    :param query: the query (list of query term id's)\n",
    "    \"\"\"\n",
    "    tfidf_score = 0.0\n",
    "    for t in query:\n",
    "        try:\n",
    "            # tf = how often does term appear in document\n",
    "            tf = inverted_index[t][doc_id]\n",
    "            idf = log(num_documents) - log(len(inverted_index[t]))\n",
    "            tfidf_score += log(1+tf)*idf\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25Score(doc_id, query, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 scoring function for a document and a query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query: the query (list of terms)\n",
    "    :param k1: (fixed at 1.2)\n",
    "    :param b: (fixed at 0.75)\n",
    "    \"\"\"\n",
    "    bm25 = 0.0\n",
    "    for q in set(query):\n",
    "        try:\n",
    "            tf = inverted_index[q][doc_id]\n",
    "            w = ((k1+1)*tf) / (k1*((1-b)+b*(avg_doc_length))+tf)\n",
    "            idf = log(num_documents) - log(len(inverted_index[q]))\n",
    "            # use logs to prevent underflow\n",
    "            bm25 += w + idf \n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-45.500192454108415\n"
     ]
    }
   ],
   "source": [
    "def constructLM(document_term_freqs, collection_term_freqs, smoothing, param):\n",
    "    '''\n",
    "    Constructs language model using several different smoothing techniques\n",
    "    \n",
    "    Jelinek-Mercer smoothing linearly interpolates using the background language model.\n",
    "    Intuively, this captures the assumption that for an unseen word, its smoothed\n",
    "    probability should depend on the frequency of its appearence over all documents. \n",
    "    LM is a multinomial distrubution (unigram). Param=0 uses only the background LM\n",
    "    \n",
    "    Dirichlet smoothing computes doc-term probabilities using both the document LM\n",
    "    as well as the background LM, and does so while taking into account\n",
    "    the document's length. The intuition behind this is that longer\n",
    "    documents need less smoothing because they can rely more on their own LM.\n",
    "    \n",
    "    Absolute discounting substracts param from each document term frequency count\n",
    "    and adds it to unseen words, thus redistributes the probability mass. \n",
    "    \n",
    "    :param document_term_freq: the document term frequencies \n",
    "    :param collection_term_freq: the collection term frequencies used for background LM\n",
    "    :param smoothing: different types of smoothing\n",
    "    :param param: decides the degree of smoothing\n",
    "    '''\n",
    "    n_doc_tokens = sum(document_term_freqs.values())\n",
    "    n_doc_types = sum(set(document_term_freqs.values()))\n",
    "    n_col_tokens = sum(collection_term_freqs.values())\n",
    "\n",
    "    document_term_freqs = collections.defaultdict(int,document_term_freqs)\n",
    "    doc_term_prob = collections.defaultdict(float)\n",
    "    \n",
    "    # Loop over vocabulary (query terms) and construct JM-smoothed document language model\n",
    "    for q_t_id in query_term_ids:\n",
    "        doc_tf = document_term_freqs[q_t_id]\n",
    "        col_tf = collection_term_freqs[q_t_id]\n",
    "        \n",
    "        if smoothing == 'JM':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = param*(doc_tf/float(n_doc_tokens)) + (1-param)*(col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "                \n",
    "        elif smoothing == 'dirichlet':\n",
    "            doc_term_prob[q_t_id] = (doc_tf + param*(col_tf/n_col_tokens)) / (n_doc_tokens+param)\n",
    "            \n",
    "        elif smoothing == 'discount':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = max(doc_tf-param,0)/n_doc_tokens + (param*n_doc_types)/n_doc_tokens * (col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "        else:\n",
    "            raise('Smoothing method not recognized!')\n",
    "\n",
    "    return doc_term_prob\n",
    "\n",
    "def queryLikelihood(query_id, LM):\n",
    "    '''\n",
    "    Calculates query likelihood given language model and query\n",
    "    \n",
    "    :param query_id: query id\n",
    "    :param LM language model\n",
    "    '''\n",
    "    if LM is None:\n",
    "        return None\n",
    "    \n",
    "    q_likelihood = 0\n",
    "    for q_t_id in tokenized_queries[query_id]:\n",
    "        try:\n",
    "            q_likelihood += log(LM[q_t_id])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return q_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "COLLECTION_FREQUENCY = sum(collection_frequencies.values())\n",
    "\n",
    "def gaussian(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Gaussian kernel function\"\"\"\n",
    "    return math.exp(-(i-j)**2/(2*sigm**2))\n",
    "\n",
    "def triangle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Triangle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1 - abs(i-j)/sigm\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def cosine(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Cosine (Hamming) kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 0.5*(1 + math.cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value    \n",
    "\n",
    "def circle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Circle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = (1 + cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value     \n",
    "    \n",
    "def passage(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Passage kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def PLM_prob(int_document_id, query_term_id, i, col_lang_model, ocs, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates estimated language model of virtual document\"\"\"\n",
    "    _, doc_token_ids = index.document(int_document_id)\n",
    "    prop_count = sum([kernel(i,j) for j in ocs])\n",
    "    N = document_lengths[int_document_id]\n",
    "    \n",
    "    sigma = 50\n",
    "    if kernel == gaussian:\n",
    "        z_i = math.sqrt(2*math.pi*sigma**2)*(norm.cdf((N-i)/float(sigma) -\n",
    "                                                      norm.cdf((1-i)/sigma)))\n",
    "    elif kernel == triangle:\n",
    "        z_i = sigma\n",
    "    elif kernel == cosine:\n",
    "        z_i = sigma\n",
    "    elif kernel == circle:\n",
    "        raise NotImplementedError\n",
    "    elif kernel == passage:\n",
    "        z_i = 2*sigma\n",
    "    \n",
    "    prob = (prop_count + mu * col_lang_model[query_term_id]/(float(z_i) + mu))\n",
    "    return prob\n",
    "\n",
    "def PLM_score(int_document_id, query, i, col_lang_model, query_lang_model, ocs, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates score for PLM at position i\"\"\"\n",
    "    score = 0\n",
    "    for query_term_id in set(query):\n",
    "        #ML Estimate of query language model        \n",
    "        score -= query_lang_model[query_term_id] * math.log(query_lang_model[query_term_id]/PLM_prob(int_document_id, \n",
    "                                                                     query_term_id, \n",
    "                                                                     i, col_lang_model, ocs, mu=mu,\n",
    "                                                                     kernel=kernel))\n",
    "    return score\n",
    "            \n",
    "def PLM(int_document_id, query, mu=50,kernel=triangle):\n",
    "    \"\"\"Scores document using PLM Best Position Strategy\"\"\"\n",
    "    N = document_lengths[int_document_id]\n",
    "    _, doc_token_ids = index.document(int_document_id)\n",
    "    best_score = 0\n",
    "    \n",
    "    query_lang_model = {}\n",
    "    col_lang_model = {}\n",
    "    ocs = {}\n",
    "    query_len = len(query)\n",
    "    for q in query:\n",
    "        query_lang_model[q] = query.count(q)/float(query_len)\n",
    "        col_lang_model[q] = collection_frequencies[q\n",
    "                                                   ]/COLLECTION_FREQUENCY\n",
    "        ocs[q] = [i+1 for i, x in enumerate(doc_token_ids) if x == q]\n",
    "    \n",
    "    for i in range(1,N+1):\n",
    "        score = PLM_score(int_document_id, query, i, col_lang_model, query_lang_model, ocs, mu=mu, kernel=kernel)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, param=0.1):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    :param param: parameter value for score_fn\n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name+str(param))\n",
    "\n",
    "    if model_name == 'lsi' or model_name == 'lsa':\n",
    "        print('creating LSM...')\n",
    "        lsm = LSM(model_name)\n",
    "        print('created LSM.')\n",
    "    \n",
    "    print('Retrieving using', model_name,'...')\n",
    "    data = collections.defaultdict(list)  \n",
    "    \n",
    "    i=5000\n",
    "    for doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        if doc_id % i == 0: print(\"Percentage completed: %.2f\" % (doc_id/index.maximum_document()))\n",
    "        \n",
    "        if model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "            LM = constructLM(docs_unigram[doc_id], collection_frequencies, model_name, param)\n",
    "         \n",
    "        for query_id, query_terms in tokenized_queries.items():\n",
    "            if model_name == 'tfidf' or model_name == 'BM25':\n",
    "                score = score_fn(doc_id, tokenized_queries[query_id])\n",
    "\n",
    "            # smoothing\n",
    "            elif model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "                score = score_fn(query_id, LM)\n",
    "\n",
    "            #use external document id (index.document(doc_ID))\n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "    \n",
    "    print('Retrieving completed.')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# translate document names to document index\n",
    "doc_id_transl = {}\n",
    "for i in range(1,index.document_count()):\n",
    "    doc_id_transl[index.ext_document_id(i)] = i\n",
    "\n",
    "# load top 1000 tf-idf\n",
    "with open('run/tfidf.run') as f:\n",
    "    top_1000 = defaultdict(list)\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        q_id, doc_name = line[0], line[2]\n",
    "        top_1000[q_id].append(doc_id_transl[doc_name])\n",
    "\n",
    "def run_1000(method, score_fn, fn, param, max_length=1000):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    Similar to run_retrieval, but uses top 1000 (or other specified value) tfidf documents.\n",
    "    Used for LSI, LDA and PLM.\n",
    "    \n",
    "    :param method: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    :param param: parameter value for score_fn\n",
    "    :param max_length: \n",
    "    \"\"\"\n",
    "    if method == 'lsi' or method == 'lda':\n",
    "        lsm = LSM(method, num_topics=param)\n",
    "    \n",
    "    i = 1\n",
    "    data = collections.defaultdict(list)\n",
    "    for query_id in top_1000.keys():\n",
    "        print('query {} from {}'.format(i, len(top_1000)))\n",
    "        i+=1\n",
    "        j = 0\n",
    "        for doc_id in top_1000[query_id][:max_length]:\n",
    "            if j % 25 == 0:\n",
    "                print('docs: {}%'.format((j/100)*100))\n",
    "            j+=1\n",
    "            if method == 'lsi' or method == 'lda':\n",
    "                score = LSM_score(query_id, doc_id, lsm, method)\n",
    "            \n",
    "            # check if query term in document, otherwise skip\n",
    "            elif method == 'PLM':\n",
    "                calculate_score = False\n",
    "                for q in tokenized_queries[query_id]:\n",
    "                    if q in set(index.document(doc_id)[1]):\n",
    "                        calculate_score = True\n",
    "                        break\n",
    "                \n",
    "                if calculate_score == True:\n",
    "                    score = score_fn(doc_id, tokenized_queries[query_id], mu=param)\n",
    "                else:\n",
    "                    score = None\n",
    "                \n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "                \n",
    "    print('Retrieving completed.')\n",
    "\n",
    "    with open(fn, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=method,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trec_eval(qrel, run, mean=True):\n",
    "    \"\"\"\n",
    "    Python wrapper function for trec_eval that uses shell scripting to obtain\n",
    "    NDCG@10, MAP@1000, Precision@5 and Recall@1000 scores. Returns dict with scores.\n",
    "        \n",
    "    :param qrel: the location of the qrel file to use\n",
    "    :param run: the location of the .run file to use\n",
    "    :param mean: if True, only the mean scores are returned.\n",
    "                 Otherwise, lists with scores per query are returned.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    nDCG = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^ndcg_cut_10 \"\n",
    "    scores['nDCG'] = np.array([float(s.split()[-1]) for s in nDCG[:-1]])\n",
    "    MAP = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^map\\s\"            \n",
    "    scores['MAP'] = np.array([float(s.split()[-1]) for s in MAP[:-1]])\n",
    "    P5 = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^P_5\\s\"\n",
    "    scores['P5'] = np.array([float(s.split()[-1]) for s in P5[:-1]])\n",
    "    recall = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^recall_1000\\s\"\n",
    "    scores['recall'] = np.array([float(s.split()[-1]) for s in recall[:-1]])\n",
    "    \n",
    "    if mean == True:\n",
    "        scores['nDCG'] = np.mean(scores['nDCG'])\n",
    "        scores['MAP'] = np.mean(scores['MAP'])\n",
    "        scores['P5'] = np.mean(scores['P5'])\n",
    "        scores['recall'] = np.mean(scores['recall'])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_retrieval('lsi', LSM_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.2195',\n",
       " 'P5': '0.3467',\n",
       " 'nDCG': '0.3586',\n",
       " 'recall': 0.6239876590821443}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'tfidf.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "# JM range 0.1-0.9\n",
    "for p in np.arange(0.1,1.0,0.1):\n",
    "    run_retrieval('JM', queryLikelihood, param=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "For the language models, create plots showing NDCG@10 with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's subprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-tailed paired Student t-test\n",
    "Compute significance of the results using a two-tailed paired Student t-test [5 points]. Be wary of false rejection of the null hypothesis caused by the multiple comparisons problem. There are multiple ways to mitigate this problem and it is up to you to choose one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    '''\n",
    "    This class creates a generator object from a pyndri index, yielding\n",
    "    a document represented as a bow or dense tfidf compatible with gensim.\n",
    "   \n",
    "    :param: method: which method to use, if lda than return a bow and otherwise\n",
    "            return a dense tfidf. If word2vec than return list of words per doc.\n",
    "    '''\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "            unigram = docs_unigram[int_doc_id]\n",
    "           \n",
    "            # return BOW representation\n",
    "            if self.method == 'lda':\n",
    "                yield list(unigram.items())\n",
    "           \n",
    "            # return TFIDF representation\n",
    "            elif self.method == 'lsi':\n",
    "                yield dense_tfidf(unigram)\n",
    "            elif self.method == 'word2vec':\n",
    "                ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "                yield tuple(dictionary[token_id] for token_id in doc_token_ids \\\n",
    "                            if token_id > 0 and token_id in dictionary)\n",
    "            else:\n",
    "                raise ValueError('Method in MyCorpus undefined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_tfidf(unigram):\n",
    "    '''\n",
    "    Transforms a document with unigram representation to a dense \n",
    "    TFIDF list. Returns a list of (token_id, tfidf) tuples.\n",
    "    '''\n",
    "    tfidf_dict = collections.defaultdict(float)\n",
    "    for token_id, tf in unigram.items():\n",
    "        idf = log(num_documents) - log(len(inverted_index[token_id]))\n",
    "        tfidf_dict[token_id] = log(1+tf)*idf\n",
    "    return list(tfidf_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def LSM(lsm_method, num_topics=10):\n",
    "    '''\n",
    "    Implements two Latent Sematic Models: LSI and LDA using gensim.\n",
    "    \n",
    "    :param method: either lsi or lda. This decides whether the\n",
    "    corpus streaming object returns a BOW or TFIDF representation\n",
    "    and whether the latent semantic model that is being returned\n",
    "    is LSI or LDA.\n",
    "    '''\n",
    "    print('creating LSM with '+str(num_topics)+' topics.')\n",
    "    \n",
    "    corpus = MyCorpus(lsm_method)\n",
    "    if lsm_method == 'lsi':\n",
    "        lsm = LsiModel(corpus, id2word = id2token, num_topics=num_topics)\n",
    "    else:\n",
    "        lsm = LdaModel(list(corpus), id2word = id2token, num_topics=num_topics)\n",
    "    return lsm\n",
    "\n",
    "def LSM_score(query_id, document_id, lsm, lsm_method):\n",
    "    '''\n",
    "    Calculate a query-document score between either LSA \n",
    "    or LDA representations of the query and document\n",
    "    using the cosine similarity.\n",
    "    '''\n",
    "    \n",
    "    # Construct query and and document representation\n",
    "    query_unigram = collections.Counter(\n",
    "        token_id for token_id in tokenized_queries[query_id]\n",
    "        if token_id > 0)\n",
    "    doc_unigram = docs_unigram[document_id]\n",
    "    \n",
    "    if lsm_method == 'lsi':\n",
    "        query_repr = lsm[dense_tfidf(query_unigram)]\n",
    "        doc_repr = lsm[dense_tfidf(doc_unigram)]\n",
    "    else:\n",
    "        query_repr = lsm[list(query_unigram.items())]\n",
    "        doc_repr = lsm[list(doc_unigram.items())]\n",
    "\n",
    "    query_repr = [[topic for idx, topic in query_repr]]\n",
    "    doc_repr = [[topic for idx, topic in doc_repr]]\n",
    "    \n",
    "    try:\n",
    "        score = cosine_similarity(query_repr, doc_repr)\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file: run/lda_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=100.run\n",
      "creating LSM with 100 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidstap/anaconda2/envs/ir1/lib/python3.5/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=100.run\n",
      "creating LSM with 100 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "for param in [10, 20, 50, 100, 200,400,600,800,1000]:\n",
    "    print('creating file: run/lda_ntopics='+str(param)+'.run')\n",
    "    run_1000(top_1000, doc_repres, 'lda', param, 'lda_ntopics='+str(param)+'.run')\n",
    "        print('creating file: run/lsi_ntopics='+str(param)+'.run')\n",
    "    run_1000(top_1000, doc_repres, 'lsi', param, 'lsi_ntopics='+str(param)+'.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval('ap_88_898_89/qrel_validation', 'run/lsi_top1000.run',mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def train_word2vec(corpus):\n",
    "    '''\n",
    "    This method uses gensim's implementation of word2vec to train it on\n",
    "    a corpus constructed from a pyndri index. It saves and returns a \n",
    "    KeyedVectors object, which, when loaded appropriately, serves as a simple\n",
    "    token to word embedding mapping.\n",
    "    \n",
    "    :param corpus: an iterable that returns a list of tokens for each document\n",
    "    '''\n",
    "    word2vec_model = Word2Vec(\n",
    "        size=300,  # Embedding size\n",
    "        window=5,  # One-sided window size\n",
    "        sg=True,  # Skip-gram.\n",
    "        min_count=5,  # Minimum word frequency.\n",
    "        sample=1e-3,  # Sub-sample threshold.\n",
    "        hs=False,  # Hierarchical softmax.\n",
    "        negative=10,  # Number of negative examples.\n",
    "        iter=5,  # Number of iterations.\n",
    "        workers=8,  # Number of workers.\n",
    "    )\n",
    "    word2vec_model.build_vocab(corpus, trim_rule=None)\n",
    "    word2vec_model.train(corpus, word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "    word_vectors = word2vec.wv\n",
    "    word_vectors.save('custom_word_vectors.txt')\n",
    "    return word_vectors\n",
    "\n",
    "corpus = MyCorpus(method = 'word2vec')\n",
    "# word_vectors = train_word2vec(corpus)\n",
    "\n",
    "# Load pre-trained word2vec embeddings from google\n",
    "word_vectors = KeyedVectors.load_word2vec_format('reduced_vectors_google.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordvecs_to_repr(word_vectors, pooling_method, doc_id = None, q_id = None):\n",
    "    '''\n",
    "    This method construct a fixed vector representation of a query\n",
    "    or a document based on their individual word embeddings obtained\n",
    "    by word2vec. There are two ways to get to those representations:\n",
    "    the first is to simple average all word vectors in the query\n",
    "    or document, the second is to take a weighted average, weighted\n",
    "    by a word's TFIDF score.\n",
    "    \n",
    "    :param word_vectors: dict that maps token to its word2vec embedding\n",
    "    :param pooling_method: tfidf or avr\n",
    "    :param doc_id/q_id: either a query or doc id\n",
    "    '''\n",
    "    \n",
    "    # Either use a query or doc_id\n",
    "    if doc_id:\n",
    "        unigram = docs_unigram[doc_id]\n",
    "    elif q_id:\n",
    "        unigram = collections.Counter(\n",
    "                token_id for token_id in tokenized_queries[q_id]\n",
    "                if token_id > 0)\n",
    "    else:\n",
    "        raise ValueError('Please specify either a doc id or a query id')\n",
    "        \n",
    "    # Start with a vector of zeros and add (weighted) word embeddings to it\n",
    "    total_weight = 0\n",
    "    weighted_bow = np.zeros(word_vectors['independent'].shape)\n",
    "    for token_id, tf in unigram.items():\n",
    "        try:\n",
    "            word_vec = word_vectors[id2token[token_id]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        if pooling_method == 'tfidf':\n",
    "            idf = log(num_documents) - log(len(inverted_index[token_id]))\n",
    "            tfidf = log(1+tf)*idf\n",
    "            total_weight += tfidf\n",
    "            weighted_bow += tfidf*word_vec\n",
    "        elif pooling_method == 'avr':\n",
    "            total_weight += 1\n",
    "            weighted_bow += word_vec\n",
    "            \n",
    "    # Return a (weighted) average\n",
    "    return weighted_bow/total_weight\n",
    "\n",
    "pooling_method = 'tfidf'\n",
    "wordvecs_to_repr(word_vectors, pooling_method, q_id='52')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
