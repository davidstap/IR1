{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, _ = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    topics = parse_topics([f_topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ùõå in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ùõç [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ùõÖ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of ‚Äúsoft‚Äù passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ùõî equal to 50, and Dirichlet smoothing with ùõç optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don‚Äôt forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "docs_unigram = collections.defaultdict(dict)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        #term frequency\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        # document frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "        # unigram model (added)\n",
    "        docs_unigram[int_doc_id][query_term_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def tfidfScore(doc_id, query):\n",
    "    \"\"\"\n",
    "    TF-IDF scoring function for a document and a query term\n",
    "    \n",
    "    :param doc_id: the document id\n",
    "    :param query: the query (list of query term id's)\n",
    "    \"\"\"\n",
    "    tfidf_score = 0.0\n",
    "    for t in query:\n",
    "        try:\n",
    "            # tf = how often does term appear in document\n",
    "            tf = inverted_index[t][doc_id]\n",
    "            idf = log(num_documents) - log(len(inverted_index[t]))\n",
    "            tfidf_score += log(1+tf)*idf\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BM25Score(doc_id, query, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 scoring function for a document and a query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query: the query (list of terms)\n",
    "    :param k1: (fixed at 1.2)\n",
    "    :param b: (fixed at 0.75)\n",
    "    \"\"\"\n",
    "    bm25 = 0.0\n",
    "    for q in set(query):\n",
    "        try:\n",
    "            tf = inverted_index[q][doc_id]\n",
    "            w = ((k1+1)*tf) / (k1*((1-b)+b*(avg_doc_length))+tf)\n",
    "            idf = log(num_documents) - log(len(inverted_index[q]))\n",
    "            # use logs to prevent underflow\n",
    "            bm25 += w + idf \n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def constructLM(document_term_freqs, collection_term_freqs, smoothing, param):\n",
    "    '''\n",
    "    Constructs language model using several different smoothing techniques\n",
    "    \n",
    "    Jelinek-Mercer smoothing linearly interpolates using the background language model.\n",
    "    Intuively, this captures the assumption that for an unseen word, its smoothed\n",
    "    probability should depend on the frequency of its appearence over all documents. \n",
    "    LM is a multinomial distrubution (unigram). Param=0 uses only the background LM\n",
    "    \n",
    "    Dirichlet smoothing computes doc-term probabilities using both the document LM\n",
    "    as well as the background LM, and does so while taking into account\n",
    "    the document's length. The intuition behind this is that longer\n",
    "    documents need less smoothing because they can rely more on their own LM.\n",
    "    \n",
    "    Absolute discounting substracts param from each document term frequency count\n",
    "    and adds it to unseen words, thus redistributes the probability mass. \n",
    "    \n",
    "    :param document_term_freq: the document term frequencies \n",
    "    :param collection_term_freq: the collection term frequencies used for background LM\n",
    "    :param smoothing: different types of smoothing\n",
    "    :param param: decides the degree of smoothing\n",
    "    '''\n",
    "    n_doc_tokens = sum(document_term_freqs.values())\n",
    "    n_doc_types = sum(set(document_term_freqs.values()))\n",
    "    n_col_tokens = sum(collection_term_freqs.values())\n",
    "\n",
    "    document_term_freqs = collections.defaultdict(int,document_term_freqs)\n",
    "    doc_term_prob = collections.defaultdict(float)\n",
    "    \n",
    "    # Loop over vocabulary (query terms) and construct JM-smoothed document language model\n",
    "    for q_t_id in query_term_ids:\n",
    "        doc_tf = document_term_freqs[q_t_id]\n",
    "        col_tf = collection_term_freqs[q_t_id]\n",
    "        \n",
    "        if smoothing == 'JM':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = param*(doc_tf/float(n_doc_tokens)) + (1-param)*(col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "                \n",
    "        elif smoothing == 'dirichlet':\n",
    "            doc_term_prob[q_t_id] = (doc_tf + param*(col_tf/n_col_tokens)) / (n_doc_tokens+param)\n",
    "            \n",
    "        elif smoothing == 'discount':\n",
    "            try:\n",
    "                doc_term_prob[q_t_id] = max(doc_tf-param,0)/n_doc_tokens + (param*n_doc_types)/n_doc_tokens * (col_tf/n_col_tokens)\n",
    "            except ZeroDivisionError:\n",
    "                return None\n",
    "        else:\n",
    "            raise('Smoothing method not recognized!')\n",
    "\n",
    "    return doc_term_prob\n",
    "\n",
    "def queryLikelihood(query_id, LM):\n",
    "    '''\n",
    "    Calculates query likelihood given language model and query\n",
    "    \n",
    "    :param query_id: query id\n",
    "    :param LM language model\n",
    "    '''\n",
    "    if LM is None:\n",
    "        return None\n",
    "    \n",
    "    q_likelihood = 0\n",
    "    for q_t_id in tokenized_queries[query_id]:\n",
    "        try:\n",
    "            q_likelihood += log(LM[q_t_id])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return q_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "COLLECTION_FREQUENCY = sum(collection_frequencies.values())\n",
    "\n",
    "def gaussian(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Gaussian kernel function\"\"\"\n",
    "    return math.exp(-(i-j)**2/(2*sigm**2))\n",
    "\n",
    "def triangle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Triangle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1 - abs(i-j)/sigm\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def cosine(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Cosine (Hamming) kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 0.5*(1 + math.cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value    \n",
    "\n",
    "def circle(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Circle kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = (1 + cos(abs(i-j)*math.pi/sigm))\n",
    "    else:\n",
    "        value = 0\n",
    "    return value     \n",
    "    \n",
    "def passage(i, j, sigm=50):\n",
    "    \"\"\"Implements proximity-based Passage kernel function\"\"\"\n",
    "    if abs(i-j) <= sigm:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def PLM_prob(int_document_id, query_term_id, i, col_lang_model, ocs, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates estimated language model of virtual document\"\"\"\n",
    "    _, doc_token_ids = index.document(int_document_id)\n",
    "    # hier loopten we niet door ocs[query_term_id] maar door ocs, de keys van de dict denk ik\n",
    "    prop_count = sum([kernel(i,j) for j in ocs[query_term_id]])\n",
    "    N = document_lengths[int_document_id]\n",
    "    \n",
    "    sigma = 50\n",
    "    if kernel == gaussian:\n",
    "        z_i = math.sqrt(2*math.pi*sigma**2)*(norm.cdf((N-i)/float(sigma) -\n",
    "                                                      norm.cdf((1-i)/sigma)))\n",
    "    elif kernel == triangle:\n",
    "        z_i = sigma\n",
    "    elif kernel == cosine:\n",
    "        z_i = sigma\n",
    "    elif kernel == circle:\n",
    "        raise NotImplementedError\n",
    "    elif kernel == passage:\n",
    "        z_i = 2*sigma\n",
    "    \n",
    "    prob = (prop_count + mu * col_lang_model[query_term_id]/(float(z_i) + mu))\n",
    "    return prob\n",
    "\n",
    "def PLM_score(int_document_id, query, i, col_lang_model, query_lang_model, ocs, mu=50, kernel=gaussian):\n",
    "    \"\"\"Calculates score for PLM at position i\"\"\"\n",
    "    score = 0\n",
    "    for query_term_id in set(query):\n",
    "        #ML Estimate of query language model        \n",
    "        score -= query_lang_model[query_term_id] * math.log(query_lang_model[query_term_id]/PLM_prob(int_document_id, \n",
    "                                                                     query_term_id, \n",
    "                                                                     i, col_lang_model, ocs, mu=mu,\n",
    "                                                                     kernel=kernel))\n",
    "    return score\n",
    "            \n",
    "def PLM(int_document_id, query, mu=50,kernel=triangle):\n",
    "    \"\"\"Scores document using PLM Best Position Strategy\"\"\"\n",
    "    N = document_lengths[int_document_id]\n",
    "    _, doc_token_ids = index.document(int_document_id)\n",
    "    best_score = 0\n",
    "    \n",
    "    query_lang_model = {}\n",
    "    col_lang_model = {}\n",
    "    ocs = {}\n",
    "    query_len = len(query)\n",
    "    for q in query:\n",
    "        query_lang_model[q] = query.count(q)/float(query_len)\n",
    "        col_lang_model[q] = collection_frequencies[q\n",
    "                                                   ]/COLLECTION_FREQUENCY\n",
    "        ocs[q] = [i+1 for i, x in enumerate(doc_token_ids) if x == q]\n",
    "    \n",
    "    for i in range(1,N+1):\n",
    "        score = PLM_score(int_document_id, query, i, col_lang_model, query_lang_model, ocs, mu=mu, kernel=kernel)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, param=0.1):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    :param param: parameter value for score_fn\n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name+str(param))\n",
    "\n",
    "    if model_name == 'lsi' or model_name == 'lsa':\n",
    "        print('creating LSM...')\n",
    "        lsm = LSM(model_name)\n",
    "        print('created LSM.')\n",
    "    \n",
    "    print('Retrieving using', model_name,'...')\n",
    "    data = collections.defaultdict(list)  \n",
    "    \n",
    "    i=5000\n",
    "    for doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        if doc_id % i == 0: print(\"Percentage completed: %.2f\" % (doc_id/index.maximum_document()))\n",
    "        \n",
    "        if model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "            LM = constructLM(docs_unigram[doc_id], collection_frequencies, model_name, param)\n",
    "         \n",
    "        for query_id, query_terms in tokenized_queries.items():\n",
    "            if model_name == 'tfidf' or model_name == 'BM25':\n",
    "                score = score_fn(doc_id, tokenized_queries[query_id])\n",
    "\n",
    "            # smoothing\n",
    "            elif model_name == 'JM' or model_name == 'dirichlet' or model_name == 'discount':\n",
    "                score = score_fn(query_id, LM)\n",
    "\n",
    "            #use external document id (index.document(doc_ID))\n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "    \n",
    "    print('Retrieving completed.')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# translate document names to document index\n",
    "doc_id_transl = {}\n",
    "for i in range(1,index.document_count()):\n",
    "    doc_id_transl[index.ext_document_id(i)] = i\n",
    "\n",
    "# load top 1000 tf-idf\n",
    "with open('run/tfidf.run') as f:\n",
    "    top_1000 = defaultdict(list)\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        q_id, doc_name = line[0], line[2]\n",
    "        top_1000[q_id].append(doc_id_transl[doc_name])\n",
    "\n",
    "def run_1000(method, score_fn, fn, param, max_length=1000):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    Similar to run_retrieval, but uses top 1000 (or other specified value) tfidf documents.\n",
    "    Used for LSI, LDA and PLM.\n",
    "    \n",
    "    :param method: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    :param param: parameter value for score_fn\n",
    "    :param max_length: \n",
    "    \"\"\"\n",
    "    if method == 'lsi' or method == 'lda':\n",
    "        lsm = LSM(method, num_topics=param)\n",
    "    \n",
    "    i = 1\n",
    "    data = collections.defaultdict(list)\n",
    "    for query_id in top_1000.keys():\n",
    "        print('query {} from {}'.format(i, len(top_1000)))\n",
    "        i+=1\n",
    "        j = 0\n",
    "        for doc_id in top_1000[query_id][:max_length]:\n",
    "            if j % 25 == 0:\n",
    "                print('docs: {}%'.format((j/100)*100))\n",
    "            j+=1\n",
    "            if method == 'lsi' or method == 'lda':\n",
    "                score = LSM_score(query_id, doc_id, lsm, method)\n",
    "            \n",
    "            # check if query term in document, otherwise skip\n",
    "            elif method == 'PLM':\n",
    "                calculate_score = False\n",
    "                for q in tokenized_queries[query_id]:\n",
    "                    if q in set(index.document(doc_id)[1]):\n",
    "                        calculate_score = True\n",
    "                        break\n",
    "                \n",
    "                if calculate_score == True:\n",
    "                    score = score_fn(doc_id, tokenized_queries[query_id], mu=param)\n",
    "                else:\n",
    "                    score = None\n",
    "                \n",
    "            if score is not None:\n",
    "                data[query_id].append((score,index.document(doc_id)[0]))\n",
    "                \n",
    "    print('Retrieving completed.')\n",
    "\n",
    "    with open(fn, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=method,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trec_eval(qrel, run, mean=True):\n",
    "    \"\"\"\n",
    "    Python wrapper function for trec_eval that uses shell scripting to obtain\n",
    "    NDCG@10, MAP@1000, Precision@5 and Recall@1000 scores. Returns dict with scores.\n",
    "        \n",
    "    :param qrel: the location of the qrel file to use\n",
    "    :param run: the location of the .run file to use\n",
    "    :param mean: if True, only the mean scores are returned.\n",
    "                 Otherwise, lists with scores per query are returned.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    nDCG = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^ndcg_cut_10 \"\n",
    "    scores['nDCG'] = np.array([float(s.split()[-1]) for s in nDCG[:-1]])\n",
    "    MAP = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^map\\s\"            \n",
    "    scores['MAP'] = np.array([float(s.split()[-1]) for s in MAP[:-1]])\n",
    "    P5 = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^P_5\\s\"\n",
    "    scores['P5'] = np.array([float(s.split()[-1]) for s in P5[:-1]])\n",
    "    recall = !trec_eval/./trec_eval -m all_trec -q {qrel} {run} | grep -E \"^recall_1000\\s\"\n",
    "    scores['recall'] = np.array([float(s.split()[-1]) for s in recall[:-1]])\n",
    "    \n",
    "    if mean == True:\n",
    "        scores['nDCG'] = np.mean(scores['nDCG'])\n",
    "        scores['MAP'] = np.mean(scores['MAP'])\n",
    "        scores['P5'] = np.mean(scores['P5'])\n",
    "        scores['recall'] = np.mean(scores['recall'])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_retrieval('lsi', LSM_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': '0.2195',\n",
       " 'P5': '0.3467',\n",
       " 'nDCG': '0.3586',\n",
       " 'recall': 0.6239876590821443}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval('ap_88_89/qrel_validation', 'tfidf.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n",
      "Retrieving using JM ...\n",
      "Percentage completed: 0.03\n",
      "Percentage completed: 0.06\n",
      "Percentage completed: 0.09\n",
      "Percentage completed: 0.12\n",
      "Percentage completed: 0.15\n",
      "Percentage completed: 0.18\n",
      "Percentage completed: 0.21\n",
      "Percentage completed: 0.24\n",
      "Percentage completed: 0.27\n",
      "Percentage completed: 0.30\n",
      "Percentage completed: 0.33\n",
      "Percentage completed: 0.36\n",
      "Percentage completed: 0.39\n",
      "Percentage completed: 0.43\n",
      "Percentage completed: 0.46\n",
      "Percentage completed: 0.49\n",
      "Percentage completed: 0.52\n",
      "Percentage completed: 0.55\n",
      "Percentage completed: 0.58\n",
      "Percentage completed: 0.61\n",
      "Percentage completed: 0.64\n",
      "Percentage completed: 0.67\n",
      "Percentage completed: 0.70\n",
      "Percentage completed: 0.73\n",
      "Percentage completed: 0.76\n",
      "Percentage completed: 0.79\n",
      "Percentage completed: 0.82\n",
      "Percentage completed: 0.85\n",
      "Percentage completed: 0.88\n",
      "Percentage completed: 0.91\n",
      "Percentage completed: 0.94\n",
      "Percentage completed: 0.97\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "# JM range 0.1-0.9\n",
    "for p in np.arange(0.1,1.0,0.1):\n",
    "    run_retrieval('JM', queryLikelihood, param=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "For the language models, create plots showing NDCG@10 with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's subprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-tailed paired Student t-test\n",
    "Compute significance of the results using a two-tailed paired Student t-test [5 points]. Be wary of false rejection of the null hypothesis caused by the multiple comparisons problem. There are multiple ways to mitigate this problem and it is up to you to choose one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    '''\n",
    "    This class creates a generator object from a pyndri index, yielding\n",
    "    a document represented as a bow or dense tfidf compatible with gensim.\n",
    "   \n",
    "    :param: method: which method to use, if lda than return a bow and otherwise\n",
    "            return a dense tfidf. If word2vec than return list of words per doc.\n",
    "    '''\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "            unigram = docs_unigram[int_doc_id]\n",
    "           \n",
    "            # return BOW representation\n",
    "            if self.method == 'lda':\n",
    "                yield list(unigram.items())\n",
    "           \n",
    "            # return TFIDF representation\n",
    "            elif self.method == 'lsi':\n",
    "                yield dense_tfidf(unigram)\n",
    "            elif self.method == 'word2vec':\n",
    "                ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "                yield tuple(dictionary[token_id] for token_id in doc_token_ids \\\n",
    "                            if token_id > 0 and token_id in dictionary)\n",
    "            else:\n",
    "                raise ValueError('Method in MyCorpus undefined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_tfidf(unigram):\n",
    "    '''\n",
    "    Transforms a document with unigram representation to a dense \n",
    "    TFIDF list. Returns a list of (token_id, tfidf) tuples.\n",
    "    '''\n",
    "    tfidf_dict = collections.defaultdict(float)\n",
    "    for token_id, tf in unigram.items():\n",
    "        idf = log(num_documents) - log(len(inverted_index[token_id]))\n",
    "        tfidf_dict[token_id] = log(1+tf)*idf\n",
    "    return list(tfidf_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def LSM(lsm_method, num_topics=10):\n",
    "    '''\n",
    "    Implements two Latent Sematic Models: LSI and LDA using gensim.\n",
    "    \n",
    "    :param method: either lsi or lda. This decides whether the\n",
    "    corpus streaming object returns a BOW or TFIDF representation\n",
    "    and whether the latent semantic model that is being returned\n",
    "    is LSI or LDA.\n",
    "    '''\n",
    "    print('creating LSM with '+str(num_topics)+' topics.')\n",
    "    \n",
    "    corpus = MyCorpus(lsm_method)\n",
    "    if lsm_method == 'lsi':\n",
    "        lsm = LsiModel(corpus, id2word = id2token, num_topics=num_topics)\n",
    "    else:\n",
    "        lsm = LdaModel(list(corpus), id2word = id2token, num_topics=num_topics)\n",
    "    return lsm\n",
    "\n",
    "def LSM_score(query_id, document_id, lsm, lsm_method):\n",
    "    '''\n",
    "    Calculate a query-document score between either LSA \n",
    "    or LDA representations of the query and document\n",
    "    using the cosine similarity.\n",
    "    '''\n",
    "    \n",
    "    # Construct query and and document representation\n",
    "    query_unigram = collections.Counter(\n",
    "        token_id for token_id in tokenized_queries[query_id]\n",
    "        if token_id > 0)\n",
    "    doc_unigram = docs_unigram[document_id]\n",
    "    \n",
    "    if lsm_method == 'lsi':\n",
    "        query_repr = lsm[dense_tfidf(query_unigram)]\n",
    "        doc_repr = lsm[dense_tfidf(doc_unigram)]\n",
    "    else:\n",
    "        query_repr = lsm[list(query_unigram.items())]\n",
    "        doc_repr = lsm[list(doc_unigram.items())]\n",
    "\n",
    "    query_repr = [[topic for idx, topic in query_repr]]\n",
    "    doc_repr = [[topic for idx, topic in doc_repr]]\n",
    "    \n",
    "    try:\n",
    "        score = cosine_similarity(query_repr, doc_repr)\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file: run/lda_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=10.run\n",
      "creating LSM with 10 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=20.run\n",
      "creating LSM with 20 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=50.run\n",
      "creating LSM with 50 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=100.run\n",
      "creating LSM with 100 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidstap/anaconda2/envs/ir1/lib/python3.5/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=100.run\n",
      "creating LSM with 100 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lda_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n",
      "creating file: run/lsi_ntopics=200.run\n",
      "creating LSM with 200 topics.\n",
      "Retrieving completed.\n"
     ]
    }
   ],
   "source": [
    "for param in [10, 20, 50, 100, 200,400,600,800,1000]:\n",
    "    print('creating file: run/lda_ntopics='+str(param)+'.run')\n",
    "    run_1000(top_1000, doc_repres, 'lda', param, 'lda_ntopics='+str(param)+'.run')\n",
    "        print('creating file: run/lsi_ntopics='+str(param)+'.run')\n",
    "    run_1000(top_1000, doc_repres, 'lsi', param, 'lsi_ntopics='+str(param)+'.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec_eval('ap_88_898_89/qrel_validation', 'run/lsi_top1000.run',mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def train_word2vec(corpus):\n",
    "    '''\n",
    "    This method uses gensim's implementation of word2vec to train it on\n",
    "    a corpus constructed from a pyndri index. It saves and returns a \n",
    "    KeyedVectors object, which, when loaded appropriately, serves as a simple\n",
    "    token to word embedding mapping.\n",
    "    \n",
    "    :param corpus: an iterable that returns a list of tokens for each document\n",
    "    '''\n",
    "    word2vec_model = Word2Vec(\n",
    "        size=300,  # Embedding size\n",
    "        window=5,  # One-sided window size\n",
    "        sg=True,  # Skip-gram.\n",
    "        min_count=5,  # Minimum word frequency.\n",
    "        sample=1e-3,  # Sub-sample threshold.\n",
    "        hs=False,  # Hierarchical softmax.\n",
    "        negative=10,  # Number of negative examples.\n",
    "        iter=5,  # Number of iterations.\n",
    "        workers=8,  # Number of workers.\n",
    "    )\n",
    "    word2vec_model.build_vocab(corpus, trim_rule=None)\n",
    "    word2vec_model.train(corpus, word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "    word_vectors = word2vec.wv\n",
    "    word_vectors.save('custom_word_vectors.txt')\n",
    "    return word_vectors\n",
    "\n",
    "corpus = MyCorpus(method = 'word2vec')\n",
    "# word_vectors = train_word2vec(corpus)\n",
    "\n",
    "# Load pre-trained word2vec embeddings from google\n",
    "word_vectors = KeyedVectors.load_word2vec_format('reduced_vectors_google.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordvecs_to_repr(word_vectors, pooling_method, doc_id = None, q_id = None):\n",
    "    '''\n",
    "    This method construct a fixed vector representation of a query\n",
    "    or a document based on their individual word embeddings obtained\n",
    "    by word2vec. There are two ways to get to those representations:\n",
    "    the first is to simple average all word vectors in the query\n",
    "    or document, the second is to take a weighted average, weighted\n",
    "    by a word's TFIDF score.\n",
    "    \n",
    "    :param word_vectors: dict that maps token to its word2vec embedding\n",
    "    :param pooling_method: tfidf or avr\n",
    "    :param doc_id/q_id: either a query or doc id\n",
    "    '''\n",
    "    \n",
    "    # Either use a query or doc_id\n",
    "    if doc_id:\n",
    "        unigram = docs_unigram[doc_id]\n",
    "    elif q_id:\n",
    "        unigram = collections.Counter(\n",
    "                token_id for token_id in tokenized_queries[q_id]\n",
    "                if token_id > 0)\n",
    "    else:\n",
    "        raise ValueError('Please specify either a doc id or a query id')\n",
    "        \n",
    "    # Start with a vector of zeros and add (weighted) word embeddings to it\n",
    "    total_weight = 0\n",
    "    weighted_bow = np.zeros(word_vectors['independent'].shape)\n",
    "    for token_id, tf in unigram.items():\n",
    "        try:\n",
    "            word_vec = word_vectors[id2token[token_id]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        if pooling_method == 'tfidf':\n",
    "            idf = log(num_documents) - log(len(inverted_index[token_id]))\n",
    "            tfidf = log(1+tf)*idf\n",
    "            total_weight += tfidf\n",
    "            weighted_bow += tfidf*word_vec\n",
    "        elif pooling_method == 'avr':\n",
    "            total_weight += 1\n",
    "            weighted_bow += word_vec\n",
    "            \n",
    "    # Return a (weighted) average\n",
    "    return weighted_bow/total_weight\n",
    "\n",
    "pooling_method = 'tfidf'\n",
    "wordvecs_to_repr(word_vectors, pooling_method, q_id='52')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAKE SURE TO RUN CELL CONTAINING doc_id_transl dict\n",
    "\"\"\"\n",
    "\n",
    "def parse_qrels(file_or_files, existing_dict=None,\n",
    "               max_topics=sys.maxsize, delimiter=' '):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "    \n",
    "    if existing_dict is None:\n",
    "        qrels = collections.defaultdict(dict)\n",
    "    else:\n",
    "        qrels = existing_dict\n",
    "    \n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            topic_id, _, ext_doc_id, rel = line.split(delimiter)\n",
    "\n",
    "            try:\n",
    "                int_doc_id = doc_id_transl[ext_doc_id]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if topic_id in qrels:\n",
    "                if int_doc_id in qrels[topic_id] and (qrels[topic_id][int_doc_id] != rel):\n",
    "                        logging.error('Duplicate topic \"%s\" (%s vs. %s).', \n",
    "                                      topic_id,\n",
    "                                      qrels[topic_id][int_doc_id],\n",
    "                                      rel)\n",
    "\n",
    "            qrels[topic_id][int_doc_id] = rel\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return qrels\n",
    "\n",
    "with open('./ap_88_89/qrel_test', 'r') as f_qrels:\n",
    "    qrels = parse_qrels([f_qrels])\n",
    "\n",
    "'''UNCOMMENT THIS IF WE WANT TO INCLUDE DATA FROM VALIDATION SET \n",
    "(INSTRUCTION SAYS TO USE TEST DATA)'''    \n",
    "# with open('./ap_88_89/qrel_validation', 'r') as f_qrels:\n",
    "#     qrels = parse_qrels([f_qrels], existing_dict=qrels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_t_f = inverted_index[1]\n",
    "smoothing = 'dirichlet'\n",
    "param = 0.1\n",
    "JM = constructLM(doc_t_f, collection_frequencies, 'JM', param)\n",
    "dirichlet = constructLM(doc_t_f, collection_frequencies, 'dirichlet', param)\n",
    "discounting = constructLM(doc_t_f, collection_frequencies, 'discount', param)\n",
    "\n",
    "\n",
    "n_features = 9\n",
    "\n",
    "def generate_features(query_id, doc_id):\n",
    "    \"\"\"OPTIONS\n",
    "    BM25\n",
    "    DIRICHLET\n",
    "    JM\n",
    "    ABSOLUTE DISCOUNTING \n",
    "    TF IDF\n",
    "    QUERY LENGTH\n",
    "    DOCUMENT LENGTH\n",
    "    NUMBER OF STOP WORDS IN QUERY AND DOCUMENT\n",
    "    \"\"\"\n",
    "    query = tokenized_queries[query_id]\n",
    "    JM_LM = constructLM(docs_unigram[doc_id], \n",
    "                     collection_frequencies, \n",
    "                     'JM', param)\n",
    "    dirichlet_LM = constructLM(docs_unigram[doc_id],\n",
    "                            collection_frequencies,\n",
    "                            'dirichlet', param)\n",
    "    discounting_LM = constructLM(docs_unigram[doc_id], \n",
    "                              collection_frequencies,\n",
    "                              'discount', param)    \n",
    "    JM = queryLikelihood(query_id, JM_LM)\n",
    "    dirichlet = queryLikelihood(query_id, dirichlet_LM)\n",
    "    discounting = queryLikelihood(query_id, discounting_LM)\n",
    "    bm25 = BM25Score(doc_id, query)\n",
    "    tfidf = tfidfScore(doc_id, query)\n",
    "    plm = PLM(doc_id, query, kernel=triangle)\n",
    "    query_len = len(query) \n",
    "    doc_len = document_lengths[doc_id]\n",
    "    _, doc_token_ids = index.document(doc_id)\n",
    "    doc_sw_perc = doc_token_ids.count(0)/len(doc_token_ids)\n",
    "    feature_vec = np.array([bm25, tfidf, plm, \n",
    "                            JM, dirichlet, discounting, \n",
    "                            query_len, doc_len, doc_sw_perc]\n",
    "                          ).reshape((1,-1))\n",
    "    return feature_vec\n",
    "\n",
    "def create_training_data(qrel_dict):\n",
    "    print('Generating training data...')\n",
    "    features = np.empty(shape=(0, n_features), dtype=np.float64)\n",
    "    targets = np.empty(shape=(0), dtype=np.int)\n",
    "    count = 0\n",
    "    for q_id, results in qrel_dict.items():\n",
    "        count += 1\n",
    "        for int_doc_id, rel in results.items():\n",
    "            fts = generate_features(q_id, int_doc_id)\n",
    "            features = np.append(features, fts, axis=0)\n",
    "            targets = np.append(targets, int(rel))\n",
    "        percentage = count/float(len(qrel_dict))*100\n",
    "        if percentage % 5 == 0:\n",
    "            print('{} % of queries used.'.format(percentage))\n",
    "    print('Complete!')\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-fa896d40582d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqrels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-9552a4b0b739>\u001b[0m in \u001b[0;36mcreate_training_data\u001b[0;34m(qrel_dict)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mint_doc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mfts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_doc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-9552a4b0b739>\u001b[0m in \u001b[0;36mgenerate_features\u001b[0;34m(query_id, doc_id)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mplm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtriangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mquery_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-380c5f63fb86>\u001b[0m in \u001b[0;36mPLM\u001b[0;34m(int_document_id, query, mu, kernel)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLM_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_lang_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_lang_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-380c5f63fb86>\u001b[0m in \u001b[0;36mPLM_score\u001b[0;34m(int_document_id, query, i, col_lang_model, query_lang_model, ocs, mu, kernel)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                                                      \u001b[0mquery_term_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                                      \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_lang_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                                                      kernel=kernel))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-380c5f63fb86>\u001b[0m in \u001b[0;36mPLM_prob\u001b[0;34m(int_document_id, query_term_id, i, col_lang_model, ocs, mu, kernel)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPLM_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_term_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_lang_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Calculates estimated language model of virtual document\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_document_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# hier loopten we niet door ocs[query_term_id] maar door ocs, de keys van de dict denk ik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features, targets = create_training_data(qrels)\n",
    "print(features.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('training_data_test.pickle', 'wb') as f:\n",
    "    training_data_dict = {'fts': features, 'tgts': targets}\n",
    "    pickle.dump(training_data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '1' '1' ..., '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "with open('training_data_valid_test.pickle', 'rb') as f:\n",
    "    training_data_dct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-07e562bda294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m scores = cross_validate(clf, features, targets, scoring=scoring, \n\u001b[0;32m----> 8\u001b[0;31m                         cv=10, return_train_score=True)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             return_times=True)\n\u001b[0;32m--> 195\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    541\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml1labs/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 43\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "    \n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = LogisticRegression()\n",
    "scores = cross_validate(clf, features, targets, scoring=scoring, \n",
    "                        cv=10, return_train_score=True)\n",
    "sorted(scores.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml1labs]",
   "language": "python",
   "name": "conda-env-ml1labs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
